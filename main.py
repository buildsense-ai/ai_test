import asyncio
import uvicorn
from fastapi import FastAPI, Request, HTTPException, UploadFile, File, Form
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List
import httpx
import json
from datetime import datetime
import socket
from enum import Enum
import io
import os
import re
import tempfile
import logging

# Document processing imports
try:
    import docx
    from PyPDF2 import PdfReader
    DOCUMENT_PROCESSING_AVAILABLE = True
except ImportError:
    DOCUMENT_PROCESSING_AVAILABLE = False

app = FastAPI(title="AI Agent Evaluation Platform", version="3.0.0")
templates = Jinja2Templates(directory="templates")

# Improved document processing functions based on user's approach
def read_docx_file(filepath: str) -> str:
    """Read Word document using direct file path approach"""
    try:
        if not DOCUMENT_PROCESSING_AVAILABLE:
            return "æ–‡æ¡£å¤„ç†åº“æœªå®‰è£…ï¼Œè¯·å®‰è£… python-docxï¼špip install python-docx"
        
        doc = docx.Document(filepath)
        # Extract text from paragraphs
        text_parts = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
        
        # Also extract text from tables
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if cell.text.strip():
                        text_parts.append(cell.text.strip())
        
        # Join and clean the text
        full_text = "\n".join(text_parts)
        cleaned_text = full_text.replace("\r", "").replace("ã€€", "").strip()
        
        print(f"ğŸ“„ Wordæ–‡æ¡£æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
        return cleaned_text
        
    except Exception as e:
        error_msg = f"Wordæ–‡æ¡£è§£æå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg

def read_pdf_file(filepath: str) -> str:
    """Read PDF document using direct file path approach"""
    try:
        if not DOCUMENT_PROCESSING_AVAILABLE:
            return "æ–‡æ¡£å¤„ç†åº“æœªå®‰è£…ï¼Œè¯·å®‰è£… PyPDF2ï¼špip install PyPDF2"
        
        with open(filepath, 'rb') as file:
            pdf_reader = PdfReader(file)
            text_parts = []
            
            for page_num, page in enumerate(pdf_reader.pages):
                text = page.extract_text()
                if text.strip():
                    text_parts.append(text.strip())
        
        # Join and clean the text
        full_text = "\n".join(text_parts)
        cleaned_text = full_text.replace("\r", "").replace("ã€€", "").strip()
        
        print(f"ğŸ“„ PDFæ–‡æ¡£æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
        return cleaned_text
        
    except Exception as e:
        error_msg = f"PDFæ–‡æ¡£è§£æå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg

def read_txt_file(filepath: str) -> str:
    """Read text file with proper encoding"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            text = f.read()
        
        # Clean the text
        cleaned_text = text.replace("\r", "").replace("ã€€", "").strip()
        
        print(f"ğŸ“„ æ–‡æœ¬æ–‡ä»¶æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
        return cleaned_text
        
    except UnicodeDecodeError:
        # Try with different encoding if UTF-8 fails
        try:
            with open(filepath, "r", encoding="gbk") as f:
                text = f.read()
            cleaned_text = text.replace("\r", "").replace("ã€€", "").strip()
            print(f"ğŸ“„ æ–‡æœ¬æ–‡ä»¶æå–æˆåŠŸ(GBKç¼–ç )ï¼Œå†…å®¹é•¿åº¦: {len(cleaned_text)} å­—ç¬¦")
            return cleaned_text
        except Exception as e:
            error_msg = f"æ–‡æœ¬æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg
    except Exception as e:
        error_msg = f"æ–‡æœ¬æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg

async def process_uploaded_document_improved(file: UploadFile) -> str:
    """Process uploaded document using improved approach with temporary files"""
    if not file or not file.filename:
        return ""
    
    # Create temporary file
    suffix = os.path.splitext(file.filename)[1].lower()
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
        try:
            # Write uploaded content to temporary file
            content = await file.read()
            tmp_file.write(content)
            tmp_file.flush()
            
            # Process based on file extension
            if suffix in ['.doc', '.docx']:
                result = read_docx_file(tmp_file.name)
            elif suffix == '.pdf':
                result = read_pdf_file(tmp_file.name)
            elif suffix == '.txt':
                result = read_txt_file(tmp_file.name)
            else:
                result = f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}ã€‚æ”¯æŒæ ¼å¼: Word (.docx), PDF (.pdf), æ–‡æœ¬ (.txt)"
            
            return result
            
        except Exception as e:
            return f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}"
        finally:
            # Clean up temporary file
            try:
                os.unlink(tmp_file.name)
            except:
                pass

# Legacy functions for backward compatibility (but using improved approach)
async def extract_text_from_docx(file_content: bytes) -> str:
    """Legacy function - now uses improved approach with temporary file"""
    with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp_file:
        try:
            tmp_file.write(file_content)
            tmp_file.flush()
            return read_docx_file(tmp_file.name)
        finally:
            try:
                os.unlink(tmp_file.name)
            except:
                pass

async def extract_text_from_pdf(file_content: bytes) -> str:
    """Legacy function - now uses improved approach with temporary file"""
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        try:
            tmp_file.write(file_content)
            tmp_file.flush()
            return read_pdf_file(tmp_file.name)
        finally:
            try:
                os.unlink(tmp_file.name)
            except:
                pass

async def process_uploaded_document(file: UploadFile) -> str:
    """Legacy function - now uses improved approach"""
    return await process_uploaded_document_improved(file)

# Core Models according to README specifications
class APIConfig(BaseModel):
    """API Configuration for AI Agent"""
    type: str = Field(default="http", description="API Type")
    url: str = Field(..., description="AI Agent API URL")
    method: str = Field(default="POST", description="HTTP Method")
    headers: Dict[str, str] = Field(default={}, description="Request Headers")
    timeout: int = Field(default=30, description="Request timeout in seconds")
    
    # Coze Agent specific fields
    agentId: Optional[str] = Field(default=None, description="Coze Agent ID")
    region: Optional[str] = Field(default="global", description="Coze region")
    
    # Coze Bot specific fields  
    botId: Optional[str] = Field(default=None, description="Coze Bot ID")
    botVersion: Optional[str] = Field(default=None, description="Coze Bot Version")

class ConversationScenario(BaseModel):
    """Test scenario for multi-turn conversation"""
    title: str = Field(..., description="Scenario title")
    turns: List[str] = Field(..., description="User conversation turns")
    context: Optional[str] = Field(default="", description="Business context")
    user_profile: Optional[str] = Field(default="", description="User persona")

class EvaluationRequest(BaseModel):
    """Main evaluation request following README structure"""
    agent_api: APIConfig = Field(..., description="AI Agent API Configuration")
    requirement_doc: Optional[str] = Field(default="", description="Requirement document for context")
    conversation_scenarios: List[ConversationScenario] = Field(..., description="Test scenarios")
    
    # Coze compatibility (temporary until full API support)
    coze_bot_id: Optional[str] = Field(default="7511993619423985674", description="Coze Bot ID for current implementation")

class EvaluationDimensions(BaseModel):
    """3-dimension evaluation framework from README"""
    fuzzy_understanding: float = Field(..., description="æ¨¡ç³Šç†è§£ä¸è¿½é—®èƒ½åŠ›")
    answer_correctness: float = Field(..., description="å›ç­”å‡†ç¡®æ€§ä¸ä¸“ä¸šæ€§") 
    persona_alignment: float = Field(..., description="ç”¨æˆ·é€‚é…åº¦")
    goal_alignment: Optional[float] = Field(default=None, description="ç›®æ ‡å¯¹é½åº¦")

class EvaluationResponse(BaseModel):
    """Response structure matching README output format"""
    evaluation_summary: Dict[str, Any] = Field(..., description="Evaluation summary")
    conversation_records: List[Dict[str, Any]] = Field(..., description="Detailed conversation records")
    recommendations: List[str] = Field(..., description="Improvement suggestions")
    timestamp: str = Field(..., description="Evaluation timestamp")

# DeepSeek Configuration
DEEPSEEK_API_KEY = "sk-d2513b4c4626409599a73ba64b2e9033"
DEEPSEEK_API_URL = "https://api.deepseek.com/chat/completions"

async def call_deepseek_api(prompt: str, max_retries: int = 2) -> str:
    """Enhanced DeepSeek API call for evaluation with suppressed exceptions"""
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,
        "max_tokens": 500
    }
    
    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(20.0)) as client:
                response = await client.post(DEEPSEEK_API_URL, headers=headers, json=payload)
                
                if response.status_code == 200:
                    result = response.json()
                    if "choices" in result and len(result["choices"]) > 0:
                        content = result["choices"][0].get("message", {}).get("content", "").strip()
                        if content and len(content) > 10:
                            return content
                
                # Suppress warning message for non-critical errors
                if attempt < max_retries - 1:
                    continue
                else:
                    return "APIè¯„ä¼°æš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨è¯„åˆ†æœºåˆ¶"
                    
        except Exception as e:
            # Suppress exception printing to avoid jarring terminal output
            if attempt < max_retries - 1:
                continue
            else:
                return "APIè¯„ä¼°æš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨è¯„åˆ†æœºåˆ¶"

async def call_ai_agent_api(api_config: APIConfig, message: str) -> str:
    """Call AI Agent API - currently supports Coze, will expand for custom APIs"""
    try:
        # Check if we should use Coze API (either explicit coze URL or fallback URL)
        if "coze" in api_config.url.lower() or "fallback" in api_config.url.lower():
            return await call_coze_api_fallback(message)
        else:
            # Generic API support (to be implemented)
            headers = api_config.headers.copy()
            headers.setdefault("Content-Type", "application/json")
            
            payload = {"message": message, "query": message}  # Generic payload format
            
            async with httpx.AsyncClient(timeout=httpx.Timeout(api_config.timeout)) as client:
                response = await client.request(
                    method=api_config.method,
                    url=api_config.url,
                    headers=headers,
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    # Try common response paths
                    if "response" in result:
                        return result["response"]
                    elif "answer" in result:
                        return result["answer"]
                    elif "message" in result:
                        return result["message"]
                    else:
                        return str(result)
                else:
                    return f"APIè°ƒç”¨å¤±è´¥: {response.status_code}"
                    
    except Exception as e:
        print(f"âŒ AI Agent APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
        return "AI Agent APIè°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®"

async def call_coze_api_fallback(message: str, bot_id: str = "7511993619423985674") -> str:
    """Fallback to Coze API for current implementation"""
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            url = "https://api.coze.cn/open_api/v2/chat"
            headers = {
                "Authorization": "Bearer pat_aWWxLQe20D8km5FsKt5W99pWL72L5LNxjkontH91q3lqqTU0ExBKUBl1cUy4tm8c",
                "Content-Type": "application/json"
            }
            
            payload = {
                "conversation_id": f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{attempt}",
                "bot_id": bot_id,
                "user": "test_user",
                "query": message,
                "stream": False
            }
            
            print(f"ğŸ”„ Coze API è°ƒç”¨å°è¯• {attempt + 1}/{max_retries}")
            
            async with httpx.AsyncClient(timeout=httpx.Timeout(60.0)) as client:
                response = await client.post(url, headers=headers, json=payload)
                
                if response.status_code == 200:
                    result = response.json()
                    messages = result.get("messages", [])
                    for msg in messages:
                        if msg.get("type") == "answer":
                            content = msg.get("content", "")
                            if content and len(content.strip()) > 0:
                                print(f"âœ… Coze API æˆåŠŸå“åº”")
                                return content
                    
                    # If no answer found, try other message types
                    for msg in messages:
                        if msg.get("content"):
                            content = msg.get("content", "")
                            if content and len(content.strip()) > 0:
                                print(f"âœ… Coze API è¿”å›å¤‡ç”¨å†…å®¹")
                                return content
                    
                    print(f"âš ï¸ Coze API è¿”å›ç©ºå†…å®¹ï¼Œå°è¯•é‡è¯•...")
                else:
                    print(f"âš ï¸ Coze API HTTPé”™è¯¯: {response.status_code}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # Exponential backoff
                        continue
                        
        except Exception as e:
            print(f"âŒ Coze APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}): {str(e)}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
                continue
    
    return "Coze APIè°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®"

def extract_score_from_response(response: str) -> float:
    """Extract numerical score from DeepSeek response"""
    try:
        # Look for patterns like "è¯„åˆ†ï¼š4åˆ†", "å¾—åˆ†ï¼š4.5", "4/5", etc.
        patterns = [
            r'è¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)',
            r'å¾—åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)',
            r'(\d+(?:\.\d+)?)\s*åˆ†',
            r'(\d+(?:\.\d+)?)\s*/\s*5',
            r'(\d+(?:\.\d+)?)\s*æ˜Ÿ'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response)
            if match:
                score = float(match.group(1))
                return min(max(score, 1.0), 5.0)  # Clamp between 1-5
        
        # If no pattern found, try to find any number
        numbers = re.findall(r'\d+(?:\.\d+)?', response)
        if numbers:
            for num in numbers:
                score = float(num)
                if 1 <= score <= 5:
                    return score
        
        # Default fallback
        return 3.0
        
    except Exception:
        return 3.0

async def call_coze_api(bot_id: str, message: str) -> str:
    """Legacy Coze Bot API call (fallback)"""
    return await call_coze_api_fallback(message, bot_id)

async def call_api(api_config: APIConfig, message: str) -> str:
    """Generic API call function"""
    return await call_ai_agent_api(api_config, message)

async def conduct_conversation(api_config: APIConfig, scenario: ConversationScenario) -> List[Dict]:
    """Conduct a conversation based on the scenario"""
    conversation_history = []
    
    print(f"ğŸ”§ API é…ç½®ç±»å‹: {api_config.type}")
    print(f"ğŸ”§ API é…ç½®è¯¦æƒ…: {api_config}")
    
    for turn_num, user_message in enumerate(scenario.turns, 1):
        print(f"  ç¬¬{turn_num}è½®: {user_message}")
        
        # Get AI response based on API type
        if api_config.type == 'coze-agent':
            print("ğŸ¤– ä½¿ç”¨ Coze Agent API")
            # Extract access token properly
            access_token = api_config.headers.get('Authorization', '')
            if access_token.startswith('Bearer '):
                access_token = access_token.replace('Bearer ', '')
            
            print(f"ğŸ”‘ Agent ID: {getattr(api_config, 'agentId', 'Not found')}")
            print(f"ğŸ”‘ Region: {getattr(api_config, 'region', 'Not found')}")
            
            ai_response = await call_coze_agent_api(
                agent_id=getattr(api_config, 'agentId', ''),
                access_token=access_token,
                message=user_message,
                region=getattr(api_config, 'region', 'global')
            )
        elif api_config.type == 'coze-bot' or hasattr(api_config, 'botId'):
            print("ğŸ¤– ä½¿ç”¨ Coze Bot API")
            bot_id = getattr(api_config, 'botId', '')
            ai_response = await call_coze_api(bot_id, user_message)
        elif api_config.type == 'custom-api':
            print("ğŸ¤– ä½¿ç”¨è‡ªå®šä¹‰ API")
            ai_response = await call_api(api_config, user_message)
        else:
            print(f"âŒ æœªçŸ¥çš„APIç±»å‹: {api_config.type}")
            ai_response = f"APIé…ç½®é”™è¯¯ï¼Œæ— æ³•è¯†åˆ«çš„APIç±»å‹: {api_config.type}"
        
        conversation_history.append({
            "turn": turn_num,
            "user_message": user_message,
            "ai_response": ai_response
        })
        
        # Truncate long responses for display
        display_response = ai_response[:100] + "..." if len(ai_response) > 100 else ai_response
        print(f"  AIå›å¤: {display_response}")
    
    return conversation_history

async def evaluate_conversation_deepseek(
    conversation_history: List[Dict], 
    scenario: Dict, 
    requirement_context: str = "",
    evaluation_mode: str = "manual",
    user_persona_info: Dict = None
) -> tuple:
    """
    Enhanced DeepSeek evaluation with persona awareness
    """
    try:
        print("ğŸ§  å¼€å§‹DeepSeekæ™ºèƒ½è¯„ä¼°...")
        
        # Build enhanced context section
        context_section = f"""
ä¸šåŠ¡åœºæ™¯: {scenario.get('context', 'é€šç”¨AIåŠ©æ‰‹åœºæ™¯')}
ç”¨æˆ·ç”»åƒ: {scenario.get('user_profile', 'æ™®é€šç”¨æˆ·')}
å¯¹è¯ä¸»é¢˜: {scenario.get('title', '')}
è¯„ä¼°æ¨¡å¼: {evaluation_mode}
"""
        
        # Add persona information if available
        if evaluation_mode == "auto" and user_persona_info:
            persona = user_persona_info.get('user_persona', {})
            context_section += f"""
æå–çš„ç”¨æˆ·è§’è‰²: {persona.get('role', '')}
ç”¨æˆ·ç»éªŒæ°´å¹³: {persona.get('experience_level', '')}
æ²Ÿé€šé£æ ¼: {persona.get('communication_style', '')}
å·¥ä½œç¯å¢ƒ: {persona.get('work_environment', '')}
"""
        
        if requirement_context:
            context_section += f"\néœ€æ±‚æ–‡æ¡£ä¸Šä¸‹æ–‡:\n{requirement_context[:1000]}"
        
        # Build conversation context
        conversation_text = ""
        for turn in conversation_history:
            conversation_text += f"ç”¨æˆ·: {turn['user_message']}\nAI: {turn['ai_response']}\n\n"
        
        # Enhanced evaluation prompts with persona awareness
        base_context = f"{context_section}\n\nå¯¹è¯è®°å½•:\n{conversation_text}\n"
        
        # Call the evaluation function
        return await perform_deepseek_evaluations({}, base_context, requirement_context)
        
    except Exception as e:
        print(f"âŒ DeepSeekè¯„ä¼°å¤±è´¥: {str(e)}")
        return {}, {}

def generate_enhanced_recommendations(evaluation_results: List[Dict], user_persona_info: Dict = None) -> List[str]:
    """
    Generate enhanced recommendations with persona awareness
    """
    recommendations = []
    
    if not evaluation_results:
        return ["âš ï¸ æ— æ³•ç”Ÿæˆå»ºè®®ï¼šè¯„ä¼°ç»“æœä¸ºç©º"]
    
    # Calculate average scores
    all_scores = {}
    for result in evaluation_results:
        scores = result.get("evaluation_scores", {})
        for dimension, score in scores.items():
            if dimension not in all_scores:
                all_scores[dimension] = []
            all_scores[dimension].append(score)
    
    avg_scores = {}
    for dimension, scores in all_scores.items():
        avg_scores[dimension] = sum(scores) / len(scores) if scores else 0
    
    overall_avg = sum(avg_scores.values()) / len(avg_scores) if avg_scores else 0
    
    # Overall performance assessment
    if overall_avg >= 4.5:
        recommendations.append("ğŸŸ¢ ä¼˜ç§€è¡¨ç°ï¼AIä»£ç†æ•´ä½“è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å„ç§ç”¨æˆ·éœ€æ±‚")
    elif overall_avg >= 4.0:
        recommendations.append("ğŸŸ¡ è‰¯å¥½è¡¨ç°ï¼AIä»£ç†åŸºæœ¬æ»¡è¶³ä½¿ç”¨éœ€æ±‚ï¼Œæœ‰è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´")
    elif overall_avg >= 3.0:
        recommendations.append("ğŸŸ  ä¸­ç­‰è¡¨ç°ï¼å»ºè®®é’ˆå¯¹ä½åˆ†ç»´åº¦è¿›è¡Œé‡ç‚¹æ”¹è¿›")
    else:
        recommendations.append("ğŸ”´ éœ€è¦æ˜¾è‘—æ”¹è¿›ï¼å»ºè®®é‡æ–°è®¾è®¡å¯¹è¯ç­–ç•¥å’ŒçŸ¥è¯†åº“")
    
    # Dimension-specific recommendations with persona awareness
    if avg_scores.get('fuzzy_understanding', 0) < 3.5:
        persona_context = ""
        if user_persona_info:
            persona_context = f"ç‰¹åˆ«æ˜¯å¯¹äº{user_persona_info['user_persona']['role']}è¿™ç±»ç”¨æˆ·ï¼Œ"
        recommendations.append(f"ğŸ’¡ æ¨¡ç³Šç†è§£èƒ½åŠ›éœ€è¦åŠ å¼ºï¼š{persona_context}å¢åŠ è¿½é—®å¼•å¯¼æœºåˆ¶ï¼Œæé«˜å¯¹ä¸æ˜ç¡®éœ€æ±‚çš„å¤„ç†èƒ½åŠ›")
    
    if avg_scores.get('answer_correctness', 0) < 3.5:
        recommendations.append("ğŸ“š ä¸“ä¸šå‡†ç¡®æ€§éœ€è¦æå‡ï¼šåŠ å¼ºçŸ¥è¯†åº“å»ºè®¾ï¼Œç¡®ä¿å›ç­”çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§")
    
    if avg_scores.get('persona_alignment', 0) < 3.5:
        persona_context = ""
        if user_persona_info:
            style = user_persona_info['user_persona']['communication_style']
            persona_context = f"ç‰¹åˆ«è¦åŒ¹é…{style}çš„æ²Ÿé€šé£æ ¼ï¼Œ"
        recommendations.append(f"ğŸ‘¥ ç”¨æˆ·åŒ¹é…åº¦æœ‰å¾…æ”¹å–„ï¼š{persona_context}ä¼˜åŒ–è¯­è¨€é£æ ¼å’Œä¸“ä¸šæœ¯è¯­ä½¿ç”¨")
    
    if avg_scores.get('goal_alignment', 0) < 3.5:
        recommendations.append("ğŸ¯ ç›®æ ‡å¯¹é½åº¦éœ€è¦æ”¹è¿›ï¼šç¡®ä¿å›ç­”èƒ½å¤Ÿæ»¡è¶³ç”¨æˆ·çš„å®é™…ä¸šåŠ¡éœ€æ±‚")
    
    # Add persona-specific recommendations
    if user_persona_info:
        persona = user_persona_info.get('user_persona', {})
        usage_context = user_persona_info.get('usage_context', {})
        
        if persona.get('work_environment'):
            recommendations.append(f"ğŸ¢ ç¯å¢ƒé€‚é…å»ºè®®ï¼šé’ˆå¯¹{persona['work_environment']}ç¯å¢ƒï¼Œä¼˜åŒ–å›ç­”çš„å®ç”¨æ€§å’Œå¯æ“ä½œæ€§")
        
        pain_points = usage_context.get('pain_points', [])
        if pain_points:
            recommendations.append(f"âš¡ ç—›ç‚¹è§£å†³ï¼šé‡ç‚¹å…³æ³¨{', '.join(pain_points[:2])}ç­‰ç”¨æˆ·ç—›ç‚¹é—®é¢˜")
    
    return recommendations

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/api/evaluate-agent-with-file", response_model=EvaluationResponse)
async def evaluate_agent_with_file(
    agent_api_config: str = Form(...),
    requirement_file: UploadFile = File(None),
    requirement_text: str = Form(None),
    conversation_scenarios: str = Form(...),
    coze_bot_id: str = Form(None),
    evaluation_mode: str = Form("manual"),  # "auto" or "manual"
    extracted_persona: str = Form(None)     # JSON string of extracted persona
):
    """
    Enhanced evaluation endpoint supporting both automatic persona extraction and manual input
    """
    try:
        print("ğŸ¤–============================================================ğŸ¤–")
        print("   AI Agent è¯„ä¼°å¹³å° v3.0 (å¢å¼ºæ¨¡å¼)")
        print("ğŸ¤–============================================================ğŸ¤–")
        
        # Parse API configuration
        try:
            api_config_dict = json.loads(agent_api_config)
            api_config = APIConfig(**api_config_dict)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"APIé…ç½®è§£æå¤±è´¥: {str(e)}")
        
        # Handle requirement document
        requirement_context = ""
        user_persona_info = None
        
        if evaluation_mode == "auto" and extracted_persona:
            # Use extracted persona information
            try:
                user_persona_info = json.loads(extracted_persona)
                print("ğŸ­ Using extracted user persona information")
            except:
                print("âš ï¸ Failed to parse extracted persona, falling back to manual mode")
                evaluation_mode = "manual"
        
        if requirement_file and requirement_file.filename:
            print(f"ğŸ“„ Processing uploaded file: {requirement_file.filename}")
            file_content = await requirement_file.read()
            
            if requirement_file.filename.endswith('.docx'):
                requirement_context = await extract_text_from_docx(file_content)
            elif requirement_file.filename.endswith('.pdf'):
                requirement_context = await extract_text_from_pdf(file_content)
            elif requirement_file.filename.endswith('.txt'):
                requirement_context = file_content.decode('utf-8', errors='ignore')
            else:
                print("âš ï¸ Unsupported file format, using text input instead")
                
        if not requirement_context and requirement_text:
            requirement_context = requirement_text
        
        # Parse conversation scenarios
        scenarios = []
        
        if evaluation_mode == "auto" and user_persona_info:
            # Auto mode: generate scenarios based on extracted persona
            print("ğŸ¯ Auto mode: Generating conversation scenarios based on extracted persona...")
            scenarios = await generate_conversation_scenarios_from_persona(user_persona_info)
            print(f"âœ… Generated {len(scenarios)} scenarios based on user persona: {user_persona_info['user_persona']['role']}")
            
        else:
            # Manual mode or fallback: parse provided scenarios
            try:
                scenarios = json.loads(conversation_scenarios)
            except Exception as e:
                if evaluation_mode == "manual":
                    raise HTTPException(status_code=400, detail=f"åœºæ™¯é…ç½®è§£æå¤±è´¥: {str(e)}")
                else:
                    # Auto mode but no valid persona - generate basic scenarios
                    print("âš ï¸ Auto mode but no persona available, generating basic scenarios...")
                    scenarios = [
                        {
                            "title": "åŸºç¡€å’¨è¯¢åœºæ™¯",
                            "context": "é€šç”¨å’¨è¯¢ç¯å¢ƒ",
                            "user_profile": "æ™®é€šç”¨æˆ·",
                            "turns": ["æˆ‘æœ‰ä¸ªé—®é¢˜", "èƒ½è¯¦ç»†è¯´æ˜ä¸€ä¸‹å—", "è°¢è°¢"]
                        }
                    ]

        if not scenarios:
            if evaluation_mode == "auto":
                raise HTTPException(status_code=400, detail="è‡ªåŠ¨æ¨¡å¼ä¸‹æ— æ³•ç”Ÿæˆå¯¹è¯åœºæ™¯ï¼Œè¯·æ£€æŸ¥ç”¨æˆ·ç”»åƒæå–æ˜¯å¦æˆåŠŸ")
            else:
                raise HTTPException(status_code=400, detail="è¯·è‡³å°‘é…ç½®ä¸€ä¸ªå¯¹è¯åœºæ™¯")

        print(f"ğŸ“‹ Total scenarios to evaluate: {len(scenarios)}")
        
        # Enhanced evaluation with persona-aware context
        evaluation_results = []
        
        for i, scenario in enumerate(scenarios, 1):
            print(f"ğŸ“‹ åœºæ™¯ {i}/{len(scenarios)}: {scenario.get('title', 'æœªå‘½ååœºæ™¯')}")
            
            # Enhance scenario with extracted persona if available
            if evaluation_mode == "auto" and user_persona_info:
                scenario = enhance_scenario_with_persona(scenario, user_persona_info)
                print(f"ğŸ­ Enhanced scenario with extracted persona: {user_persona_info['user_persona']['role']}")
            
            result = await evaluate_single_conversation_scenario(
                api_config=api_config,
                scenario=scenario,
                requirement_context=requirement_context,
                evaluation_mode=evaluation_mode,
                user_persona_info=user_persona_info
            )
            
            if result:
                evaluation_results.append(result)
            else:
                print(f"âš ï¸ åœºæ™¯ {i} è¯„ä¼°å¤±è´¥ï¼Œè·³è¿‡")

        if not evaluation_results:
            raise HTTPException(status_code=500, detail="æ‰€æœ‰åœºæ™¯è¯„ä¼°å‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥AI Agenté…ç½®")

        # Generate summary
        summary = generate_evaluation_summary(evaluation_results, requirement_context)
        
        # Enhanced recommendations with persona awareness
        recommendations = generate_enhanced_recommendations(evaluation_results, user_persona_info)
        
        response_data = {
            "evaluation_summary": summary,
            "conversation_records": evaluation_results,
            "recommendations": recommendations,
            "evaluation_mode": evaluation_mode,
            "user_persona_info": user_persona_info,
                "timestamp": datetime.now().isoformat()
        }
        
        print(f"ğŸ¯ æ€»ä½“è¯„ä¼°å®Œæˆï¼ç»¼åˆå¾—åˆ†: {summary.get('overall_score', 0):.2f}/5.0")
        print(f"ğŸ“Š è¯„ä¼°æ¨¡å¼: {evaluation_mode}")
        if user_persona_info:
            print(f"ğŸ­ ç”¨æˆ·ç”»åƒ: {user_persona_info['user_persona']['role']}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Evaluation failed with error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")

@app.post("/api/evaluate-agent-dynamic", response_model=EvaluationResponse)
async def evaluate_agent_dynamic(
    agent_api_config: str = Form(...),
    requirement_file: UploadFile = File(None),
    requirement_text: str = Form(None),
    extracted_persona: str = Form(None)  # JSON string of extracted persona
):
    """
    New dynamic evaluation endpoint implementing conversational workflow:
    1. Extract persona from document
    2. Generate 2 scenarios with dynamic conversations
    3. Conduct 2-3 rounds per scenario based on AI responses
    4. Generate comprehensive final report
    """
    try:
        print("ğŸš€============================================================ğŸš€")
        print("   AI Agent åŠ¨æ€å¯¹è¯è¯„ä¼°å¹³å° v4.0")
        print("ğŸš€============================================================ğŸš€")
        
        # Parse API configuration
        try:
            api_config_dict = json.loads(agent_api_config)
            api_config = APIConfig(**api_config_dict)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"APIé…ç½®è§£æå¤±è´¥: {str(e)}")
        
        # Handle requirement document
        requirement_context = ""
        user_persona_info = None
        
        # Step 1: Process requirement document and extract persona
        if requirement_file and requirement_file.filename:
            print(f"ğŸ“„ Processing uploaded file: {requirement_file.filename}")
            requirement_context = await process_uploaded_document_improved(requirement_file)
        elif requirement_text:
            requirement_context = requirement_text
        
        if not requirement_context:
            raise HTTPException(status_code=400, detail="è¯·æä¾›éœ€æ±‚æ–‡æ¡£æˆ–æ–‡æœ¬å†…å®¹")
        
        # Step 2: Extract user persona from requirement document using DeepSeek
        if extracted_persona:
            try:
                user_persona_info = json.loads(extracted_persona)
                print(f"ğŸ­ ä½¿ç”¨æå–çš„ç”¨æˆ·ç”»åƒ: {user_persona_info.get('user_persona', {}).get('role', 'æœªçŸ¥è§’è‰²')}")
            except:
                print("âš ï¸ ç”»åƒæ•°æ®è§£æå¤±è´¥ï¼Œé‡æ–°æå–...")
                user_persona_info = None
        
        if not user_persona_info:
            print("ğŸ§  ä»éœ€æ±‚æ–‡æ¡£ä¸­æå–ç”¨æˆ·ç”»åƒ...")
            user_persona_info = await extract_user_persona_with_deepseek(requirement_context)
            if not user_persona_info:
                raise HTTPException(status_code=400, detail="æ— æ³•ä»éœ€æ±‚æ–‡æ¡£ä¸­æå–æœ‰æ•ˆçš„ç”¨æˆ·ç”»åƒä¿¡æ¯")
        
        # Step 3: Conduct dynamic multi-scenario evaluation
        print("ğŸ¯ å¼€å§‹åŠ¨æ€å¤šè½®å¯¹è¯è¯„ä¼°...")
        evaluation_results = await conduct_dynamic_multi_scenario_evaluation(
            api_config, user_persona_info, requirement_context
        )
        
        if not evaluation_results:
            raise HTTPException(status_code=500, detail="åŠ¨æ€å¯¹è¯è¯„ä¼°å¤±è´¥ï¼Œè¯·æ£€æŸ¥AI Agenté…ç½®")
        
        # Step 4: Generate comprehensive final report
        print("ğŸ“Š ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
        comprehensive_report = await generate_final_comprehensive_report(
            evaluation_results, user_persona_info, requirement_context
        )
        
        # Calculate overall summary
        overall_score = sum(r.get('scenario_score', 0) for r in evaluation_results) / len(evaluation_results) if evaluation_results else 0
        total_conversations = sum(len(r.get('conversation_history', [])) for r in evaluation_results)
        
        response_data = {
            "evaluation_summary": {
                "overall_score": overall_score,
                "total_scenarios": len(evaluation_results),
                "total_conversations": total_conversations,
                "framework": "åŠ¨æ€å¤šè½®å¯¹è¯è¯„ä¼°",
                "comprehensive_analysis": comprehensive_report,
                "extracted_persona_display": {
                    "user_role": user_persona_info.get('user_persona', {}).get('role', 'ä¸“ä¸šç”¨æˆ·'),
                    "business_domain": user_persona_info.get('usage_context', {}).get('business_domain', 'ä¸“ä¸šæœåŠ¡'),
                    "experience_level": user_persona_info.get('user_persona', {}).get('experience_level', 'ä¸­ç­‰ç»éªŒ'),
                    "communication_style": user_persona_info.get('user_persona', {}).get('communication_style', 'ä¸“ä¸šæ²Ÿé€š'),
                    "work_environment": user_persona_info.get('user_persona', {}).get('work_environment', 'ä¸“ä¸šå·¥ä½œç¯å¢ƒ'),
                    "extraction_method": "DeepSeekæ™ºèƒ½æå–åˆ†æ"
                }
            },
            "conversation_records": evaluation_results,
            "recommendations": comprehensive_report.get('improvement_recommendations', []),
            "extracted_persona_full": comprehensive_report.get('extracted_persona_summary', {}),
            "persona_alignment_analysis": comprehensive_report.get('persona_alignment_analysis', ''),
            "business_goal_achievement": comprehensive_report.get('business_goal_achievement', ''),
            "evaluation_mode": "dynamic",
            "user_persona_info": user_persona_info,
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"ğŸ¯ åŠ¨æ€è¯„ä¼°å®Œæˆï¼ç»¼åˆå¾—åˆ†: {overall_score:.2f}/5.0")
        print(f"ğŸ“Š è¯„ä¼°åœºæ™¯: {len(evaluation_results)} ä¸ª")
        print(f"ğŸ’¬ å¯¹è¯è½®æ¬¡: {total_conversations} è½®")
        print(f"ğŸ­ ç”¨æˆ·ç”»åƒ: {user_persona_info.get('user_persona', {}).get('role', 'æœªçŸ¥è§’è‰²')}")
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Dynamic evaluation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åŠ¨æ€è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")

def enhance_scenario_with_persona(scenario: Dict, user_persona_info: Dict) -> Dict:
    """
    Enhance conversation scenario with extracted user persona information
    """
    enhanced_scenario = scenario.copy()
    
    persona = user_persona_info.get('user_persona', {})
    context = user_persona_info.get('usage_context', {})
    ai_role = user_persona_info.get('ai_role_simulation', {})
    
    # Enhance user profile
    if not scenario.get('user_profile') or scenario.get('user_profile') == '':
        enhanced_scenario['user_profile'] = f"{persona.get('role', 'ä¸“ä¸šç”¨æˆ·')}ï¼Œ{persona.get('experience_level', 'æœ‰ç»éªŒ')}"
    
    # Enhance context if not provided
    if not scenario.get('context') or scenario.get('context') == '':
        work_env = persona.get('work_environment', '')
        business_domain = context.get('business_domain', '')
        enhanced_scenario['context'] = f"{work_env} - {business_domain}" if work_env and business_domain else work_env or business_domain or "ä¸“ä¸šå·¥ä½œç¯å¢ƒ"
    
    # Add persona-aware conversation approach
    enhanced_scenario['conversation_approach'] = ai_role.get('conversation_approach', 'ç›´æ¥ä¸“ä¸šæé—®')
    enhanced_scenario['language_style'] = ai_role.get('language_characteristics', 'ä¸“ä¸šæœ¯è¯­ä¸é€šä¿—è§£é‡Šç»“åˆ')
    
    return enhanced_scenario

async def evaluate_single_conversation_scenario(
    api_config: APIConfig,
    scenario: Dict,
    requirement_context: str = "",
    evaluation_mode: str = "manual",
    user_persona_info: Dict = None
) -> Optional[Dict]:
    """
    Enhanced single scenario evaluation with persona awareness
    """
    try:
        print(f"ğŸ—£ï¸ å¼€å§‹å¯¹è¯åœºæ™¯: {scenario.get('title', 'æœªå‘½ååœºæ™¯')}")
        
        conversation_history = []
        turns = scenario.get('turns', [])
        
        if not turns:
            print("âš ï¸ åœºæ™¯æ²¡æœ‰é…ç½®å¯¹è¯è½®æ¬¡")
            return None
            
        # Enhanced conversation simulation with persona context
        for turn_num, user_message in enumerate(turns, 1):
            if not user_message.strip():
                continue
                
            print(f"ğŸ’¬ ç¬¬ {turn_num} è½®å¯¹è¯: {user_message[:50]}...")
            
            # Add persona context to the message if in auto mode
            enhanced_message = user_message
            if evaluation_mode == "auto" and user_persona_info:
                persona_context = f"[ä½œä¸º{user_persona_info['user_persona']['role']}ï¼Œ{user_persona_info['user_persona']['communication_style']}] {user_message}"
                enhanced_message = persona_context
            
            try:
                ai_response = await call_ai_agent_api(api_config, enhanced_message)
                
                if ai_response:
                    conversation_history.append({
                        "turn": turn_num,
                        "user_message": user_message,  # Store original message for display
                        "enhanced_message": enhanced_message if evaluation_mode == "auto" else user_message,
                        "ai_response": ai_response
                    })
                    print(f"âœ… AIå“åº”: {ai_response[:100]}...")
                else:
                    print(f"âŒ ç¬¬ {turn_num} è½®AIæ— å“åº”")
                    
            except Exception as e:
                print(f"âŒ ç¬¬ {turn_num} è½®å¯¹è¯å¤±è´¥: {str(e)}")
                continue
        
        if not conversation_history:
            print("âŒ åœºæ™¯å¯¹è¯å®Œå…¨å¤±è´¥")
            return None
        
        # Enhanced evaluation with persona awareness
        evaluation_scores, explanations = await evaluate_conversation_with_deepseek(
            conversation_history, scenario, requirement_context, user_persona_info
        )
        
        scenario_score = sum(evaluation_scores.values()) / len(evaluation_scores) if evaluation_scores else 0
        print(f"ğŸ¯ åœºæ™¯å¾—åˆ†: {scenario_score:.2f}/5.0")
        
        return {
            "scenario": {
                "title": scenario.get('title', 'æœªå‘½ååœºæ™¯'),
                "context": f"{user_persona_info.get('usage_context', {}).get('business_domain', 'ä¸“ä¸šæœåŠ¡') if user_persona_info else scenario.get('context', 'ä¸“ä¸šå·¥ä½œç¯å¢ƒ')} - {scenario.get('context', 'ä¸“ä¸šå·¥ä½œç¯å¢ƒ')}",
                "user_profile": f"{user_persona_info.get('user_persona', {}).get('role', 'ç”¨æˆ·') if user_persona_info else scenario.get('user_profile', 'ç”¨æˆ·')}ï¼Œ{user_persona_info.get('user_persona', {}).get('experience_level', 'ä¸­ç­‰ç»éªŒ') if user_persona_info else 'ä¸­ç­‰ç»éªŒ'}ï¼Œ{user_persona_info.get('user_persona', {}).get('communication_style', 'ä¸“ä¸šæ²Ÿé€š') if user_persona_info else 'ä¸“ä¸šæ²Ÿé€š'}"
            },
            "conversation_history": conversation_history,
            "evaluation_scores": evaluation_scores,
            "explanations": explanations,
            "scenario_score": scenario_score,
            "evaluation_mode": evaluation_mode,
            "persona_enhanced": evaluation_mode == "auto" and user_persona_info is not None,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ åœºæ™¯è¯„ä¼°å¼‚å¸¸: {str(e)}")
        return None

async def perform_deepseek_evaluations(evaluation_prompts: Dict, base_context: str, requirement_context: str) -> tuple:
    """
    Perform DeepSeek evaluations for all dimensions
    """
    try:
        # Standard evaluation prompts 
        if not evaluation_prompts:  # If prompts not provided, create them
            evaluation_prompts = {
                "fuzzy_understanding": f"""
{base_context}

è¯·è¯„ä¼°AIåœ¨æ¨¡ç³Šç†è§£ä¸è¿½é—®èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚

è¯„åˆ†æ ‡å‡† (1-5åˆ†):
1åˆ†: å®Œå…¨æ— æ³•ç†è§£æ¨¡ç³Šè¡¨è¾¾ï¼Œç›´æ¥ç»™å‡ºé”™è¯¯æˆ–æ— å…³å›ç­”
2åˆ†: ç†è§£é”™è¯¯ä¸”æœªä¸»åŠ¨è¿½é—®ï¼Œå¯èƒ½è¯¯å¯¼ç”¨æˆ·
3åˆ†: éƒ¨åˆ†ç†è§£ä½†å¼•å¯¼ä¸è¶³ï¼Œä»…ç»™å‡ºéƒ¨åˆ†æœ‰ç”¨ä¿¡æ¯
4åˆ†: åŸºæœ¬ç†è§£æ¨¡ç³Šè¡¨è¾¾ä¸”æœ‰ä¸€å®šå¼•å¯¼ï¼Œä½†è¿½é—®ä¸å¤Ÿæ·±å…¥
5åˆ†: å‡†ç¡®ç†è§£æ¨¡ç³Šè¡¨è¾¾å¹¶æœ‰æ•ˆå¼•å¯¼ç”¨æˆ·æ¾„æ¸…éœ€æ±‚

è¯·ç»™å‡ºå…·ä½“è¯„åˆ†å’Œè¯¦ç»†ç†ç”±ã€‚
""",
                "answer_correctness": f"""
{base_context}

è¯·è¯„ä¼°AIå›ç­”çš„å‡†ç¡®æ€§ä¸ä¸“ä¸šæ€§ã€‚

è¯„åˆ†æ ‡å‡† (1-5åˆ†):
1åˆ†: å›ç­”é”™è¯¯ï¼ŒåŒ…å«å±é™©ä¿¡æ¯æˆ–ä¸¥é‡è¯¯å¯¼
2åˆ†: è¡¨é¢çœ‹èµ·æ¥åˆç†ä½†æ ¸å¿ƒå†…å®¹é”™è¯¯
3åˆ†: å¤§éƒ¨åˆ†æ­£ç¡®ä½†æœ‰æ˜æ˜¾ç¼ºæ¼æˆ–ä¸å¤Ÿå‡†ç¡®
4åˆ†: åŸºæœ¬å‡†ç¡®ä¸“ä¸šä½†ç¼ºå°‘è§„èŒƒå¼•ç”¨æˆ–ç»†èŠ‚
5åˆ†: å®Œå…¨å‡†ç¡®ä¸”ä¸“ä¸šï¼Œæœ‰è§„èŒƒä¾æ®å’Œå®ç”¨æŒ‡å¯¼

è¯·ç»™å‡ºå…·ä½“è¯„åˆ†å’Œè¯¦ç»†ç†ç”±ã€‚
""",
                "persona_alignment": f"""
{base_context}

è¯·è¯„ä¼°AIä¸ç”¨æˆ·ç”»åƒçš„åŒ¹é…åº¦ã€‚

è¯„åˆ†æ ‡å‡† (1-5åˆ†):
1åˆ†: æ²Ÿé€šé£æ ¼å®Œå…¨ä¸ç¬¦åˆç”¨æˆ·èƒŒæ™¯
2åˆ†: ç”¨è¯è¿‡äºä¸“ä¸šæˆ–è¿‡äºç®€å•ï¼Œç”¨æˆ·éš¾ä»¥ç†è§£
3åˆ†: åŸºæœ¬å¯ä»¥ç†è§£ä½†å­˜åœ¨æœ¯è¯­ä½¿ç”¨ä¸å½“
4åˆ†: æ²Ÿé€šé£æ ¼åŸºæœ¬åˆé€‚ï¼Œå¶æœ‰ä¸åŒ¹é…
5åˆ†: å®Œå…¨è´´åˆç”¨æˆ·è§’è‰²å’Œæ²Ÿé€šåå¥½

è¯·ç»™å‡ºå…·ä½“è¯„åˆ†å’Œè¯¦ç»†ç†ç”±ã€‚
"""
            }
            
            # Add goal alignment if requirement context exists
            if requirement_context.strip():
                evaluation_prompts["goal_alignment"] = f"""
{base_context}

åŸºäºæä¾›çš„éœ€æ±‚æ–‡æ¡£ï¼Œè¯·è¯„ä¼°AIæ˜¯å¦è¾¾æˆäº†é¢„æœŸçš„ç›®æ ‡å¯¹é½åº¦ã€‚

è¯„åˆ†æ ‡å‡† (1-5åˆ†):
1åˆ†: å®Œå…¨åç¦»éœ€æ±‚ç›®æ ‡ï¼Œæœªè§£å†³ä»»ä½•å…³é”®é—®é¢˜
2åˆ†: éƒ¨åˆ†ç›¸å…³ä½†æœªè¾¾æˆä¸»è¦ç›®æ ‡
3åˆ†: åŸºæœ¬ç¬¦åˆéœ€æ±‚ä½†æœ‰é‡è¦é—æ¼
4åˆ†: å¾ˆå¥½åœ°æ»¡è¶³äº†å¤§éƒ¨åˆ†éœ€æ±‚ç›®æ ‡
5åˆ†: å®Œç¾å¯¹é½æ‰€æœ‰éœ€æ±‚ç›®æ ‡ï¼Œè¶…å‡ºé¢„æœŸ

è¯·ç»™å‡ºå…·ä½“è¯„åˆ†å’Œè¯¦ç»†ç†ç”±ã€‚
"""
        
        # Execute evaluations concurrently for better performance
        evaluation_results = {}
        explanations = {}
        
        for dimension, prompt in evaluation_prompts.items():
            try:
                print(f"  ğŸ“Š Evaluating {dimension}...")
                response = await call_deepseek_api(prompt)
                score = extract_score_from_response(response)
                
                evaluation_results[dimension] = score
                explanations[dimension] = response
                
                print(f"  âœ… {dimension}: {score}/5")
                
            except Exception as e:
                print(f"  âŒ Failed to evaluate {dimension}: {str(e)}")
                evaluation_results[dimension] = 3.0  # Default score
                explanations[dimension] = f"è¯„ä¼°å¤±è´¥: {str(e)}"
        
        return evaluation_results, explanations
        
    except Exception as e:
        print(f"âŒ Evaluation process failed: {str(e)}")
        return {}, {}

def generate_evaluation_summary(evaluation_results: List[Dict], requirement_context: str = "") -> Dict:
    """
    Generate evaluation summary from results
    """
    if not evaluation_results:
        return {
            "overall_score": 0.0,
            "total_scenarios": 0,
            "total_conversations": 0,
            "framework": "è¯„ä¼°å¤±è´¥",
            "dimensions": {}
        }
        
        # Calculate dimension averages
    all_scores = {}
    total_conversations = 0
    
    for result in evaluation_results:
        scores = result.get("evaluation_scores", {})
        conversation_history = result.get("conversation_history", [])
        total_conversations += len(conversation_history)
        
        for dimension, score in scores.items():
            if dimension not in all_scores:
                all_scores[dimension] = []
            all_scores[dimension].append(score)
    
    # Calculate averages
    dimension_averages = {}
    for dimension, scores in all_scores.items():
        dimension_averages[dimension] = sum(scores) / len(scores) if scores else 0
    
    overall_score = sum(dimension_averages.values()) / len(dimension_averages) if dimension_averages else 0
    
    return {
            "overall_score": round(overall_score, 2),
        "total_scenarios": len(evaluation_results),
        "total_conversations": total_conversations,
        "framework": "AI Agent 3ç»´åº¦è¯„ä¼°æ¡†æ¶",
        "dimensions": dimension_averages
    }

def generate_enhanced_recommendations(evaluation_results: List[Dict], user_persona_info: Dict = None) -> List[str]:
    """
    Generate enhanced recommendations based on evaluation results and user persona
    """
    recommendations = []
    
    if not evaluation_results:
        return [
            "æ— æ³•ç”Ÿæˆæ¨èå»ºè®®ï¼Œè¯·å…ˆå®Œæˆæœ‰æ•ˆçš„è¯„ä¼°",
            "æ£€æŸ¥AI Agenté…ç½®å’Œç½‘ç»œè¿æ¥",
            "ç¡®ä¿å¯¹è¯åœºæ™¯é…ç½®æ­£ç¡®"
        ]
    
    # Analyze scores to identify weak areas
    all_scores = {}
    for result in evaluation_results:
        scores = result.get("evaluation_scores", {})
        for dimension, score in scores.items():
            if dimension not in all_scores:
                all_scores[dimension] = []
            all_scores[dimension].append(score)
    
    # Calculate averages and identify weak points
    weak_areas = []
    for dimension, scores in all_scores.items():
        avg_score = sum(scores) / len(scores) if scores else 0
        if avg_score < 3.0:
            weak_areas.append((dimension, avg_score))
    
    # Sort by score (weakest first)
    weak_areas.sort(key=lambda x: x[1])
    
    # Generate specific recommendations based on weak areas
    dimension_recommendations = {
        "fuzzy_understanding": [
            "åŠ å¼ºAIå¯¹æ¨¡ç³Šé—®é¢˜çš„ç†è§£èƒ½åŠ›",
            "å¢å¼ºä¸»åŠ¨è¿½é—®å’Œæ¾„æ¸…éœ€æ±‚çš„æœºåˆ¶",
            "æé«˜å¯¹ç”¨æˆ·æ„å›¾çš„æ¨ç†å‡†ç¡®æ€§"
        ],
        "answer_correctness": [
            "æå‡å›ç­”çš„å‡†ç¡®æ€§å’Œä¸“ä¸šåº¦",
            "åŠ å¼ºçŸ¥è¯†åº“çš„å®Œæ•´æ€§å’Œæ—¶æ•ˆæ€§",
            "å¢åŠ è§„èŒƒå’Œæ ‡å‡†çš„å¼•ç”¨æ”¯æŒ"
        ],
        "persona_alignment": [
            "ä¼˜åŒ–AIçš„æ²Ÿé€šé£æ ¼é€‚é…èƒ½åŠ›",
            "æ ¹æ®ç”¨æˆ·èƒŒæ™¯è°ƒæ•´å›ç­”çš„å¤æ‚åº¦",
            "æé«˜å¯¹ä¸åŒç”¨æˆ·ç¾¤ä½“çš„ä¸ªæ€§åŒ–æœåŠ¡"
        ],
        "goal_alignment": [
            "æ›´å¥½åœ°ç†è§£å’Œå¯¹é½ä¸šåŠ¡ç›®æ ‡",
            "å¢å¼ºå¯¹éœ€æ±‚æ–‡æ¡£çš„æ·±åº¦ç†è§£",
            "æé«˜è§£å†³æ–¹æ¡ˆçš„é’ˆå¯¹æ€§å’Œå®ç”¨æ€§"
        ]
    }
    
    # Add recommendations for weak areas
    for dimension, score in weak_areas[:3]:  # Focus on top 3 weak areas
        if dimension in dimension_recommendations:
            recommendations.extend(dimension_recommendations[dimension])
    
    # Add persona-specific recommendations if available
    if user_persona_info:
        persona = user_persona_info.get('user_persona', {})
        role = persona.get('role', '')
        
        if 'å·¥ç¨‹å¸ˆ' in role:
            recommendations.append("å¢å¼ºå¯¹æŠ€æœ¯è§„èŒƒå’Œæ ‡å‡†çš„å¼•ç”¨èƒ½åŠ›")
        elif 'å®¢æœ' in role:
            recommendations.append("ä¼˜åŒ–å®¢æˆ·æœåŠ¡åœºæ™¯çš„å“åº”æ•ˆç‡")
        elif 'ç®¡ç†' in role:
            recommendations.append("æä¾›æ›´å¤šå†³ç­–æ”¯æŒå’Œæ•°æ®åˆ†æ")
        
        communication_style = persona.get('communication_style', '')
        if 'ç®€æ´' in communication_style:
            recommendations.append("ä¼˜åŒ–å›ç­”çš„ç®€æ´æ€§ï¼Œçªå‡ºæ ¸å¿ƒè¦ç‚¹")
        elif 'è¯¦ç»†' in communication_style:
            recommendations.append("æä¾›æ›´è¯¦ç»†çš„è§£é‡Šå’ŒèƒŒæ™¯ä¿¡æ¯")
    
    # Add general improvement suggestions
    if len(recommendations) < 3:
        recommendations.extend([
            "åŠ å¼ºå¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›",
            "æé«˜å›ç­”çš„é€»è¾‘æ€§å’Œç»“æ„åŒ–ç¨‹åº¦",
            "å¢å¼ºå¯¹ç‰¹æ®Šæƒ…å†µå’Œè¾¹ç•Œæ¡ä»¶çš„å¤„ç†"
        ])
    
    # Limit to 5 recommendations and ensure uniqueness
    unique_recommendations = list(dict.fromkeys(recommendations))
    return unique_recommendations[:5]

# Keep the old API endpoint for backward compatibility
@app.post("/api/evaluate-agent")
async def evaluate_agent_legacy(request: dict):
    """
    Legacy API endpoint for backward compatibility
    """
    try:
        print("ğŸ”„ Legacy API endpoint called, redirecting to new implementation...")
        
        # Convert old format to new format
        agent_api_config = json.dumps(request.get("agent_api", {}))
        requirement_doc = request.get("requirement_doc", "")
        conversation_scenarios = json.dumps(request.get("conversation_scenarios", []))
        
        # Call the new implementation
        return await evaluate_agent_with_file(
            agent_api_config=agent_api_config,
            requirement_file=None,
            requirement_text=requirement_doc,
            conversation_scenarios=conversation_scenarios,
            coze_bot_id=request.get("coze_bot_id"),
            evaluation_mode="manual",
            extracted_persona=None
        )
        
    except Exception as e:
        print(f"âŒ Legacy API call failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Legacy API evaluation failed: {str(e)}")

# Temporary endpoint for Coze compatibility (will be phased out)
@app.post("/evaluate-coze-auto")
async def evaluate_coze_auto(bot_id: str = None):
    """Temporary Coze compatibility endpoint"""
    try:
        # Import the legacy function temporarily
        from coze_auto_test import run_simplified_evaluation
        
        report = await run_simplified_evaluation(bot_id)
        
        # Return in the format expected by current frontend
        return {
            "success": True,
            "report": report,  # This contains evaluation_summary
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/api/schema")
async def get_api_schema():
    """Get API schema for integration"""
    return {
        "evaluation_request_example": {
            "agent_api": {
                "type": "http",
                "url": "https://your-agent.com/api/converse",
                "method": "POST"
            },
            "requirement_doc": "åœ¨å¢™é¢æŠ¹ç°æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒAIåº”è¯†åˆ«å‡ºç”¨æˆ·æ¨¡ç³Šæè¿°å¹¶ä¸»åŠ¨å¼•å¯¼è¡¥å……é¢ç§¯ã€ä½ç½®ã€è´£ä»»æ–¹ç­‰å­—æ®µ...",
            "conversation_scenarios": [
                {
                    "title": "é«˜å±‚å»ºç­‘å¢™é¢ç©ºé¼“é—®é¢˜",
                    "turns": [
                        "ä¸‰æ¥¼æœ‰ä¸ªåœ°æ–¹ç©ºé¼“äº†",
                        "æ˜¯å¢™é¢",
                        "å·®ä¸å¤šä¸¤å¹³ç±³"
                    ]
                }
            ]
        },
        "response_format": {
            "evaluation_summary": {
                "overall_score": 4.67,
                "dimensions": {
                    "fuzzy_understanding": 5,
                    "answer_correctness": 4,
                    "persona_alignment": 5,
                    "goal_alignment": 4
                }
            }
        }
    }

def find_available_port(start_port: int = 8000, max_port: int = 8010) -> int:
    """Find an available port starting from start_port"""
    for port in range(start_port, max_port + 1):
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex(('localhost', port))
            sock.close()
            if result != 0:
                return port
        except:
            continue
    return start_port

async def call_coze_agent_api(agent_id: str, access_token: str, message: str, region: str = "global") -> str:
    """Call Coze Agent API using v3 endpoint with polling for completion"""
    max_retries = 3
    
    # Determine API base URL - fix region detection
    base_url = "https://api.coze.cn" if region == "china" else "https://api.coze.com"
    print(f"ğŸŒ Using region: {region}, base URL: {base_url}")
    
    for attempt in range(max_retries):
        try:
            chat_url = f"{base_url}/v3/chat"
            headers = {
                "Authorization": f"Bearer {access_token}",
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            
            # Fix: Use proper agent_id instead of bot_id for v3 chat
            payload = {
                "bot_id": agent_id,  # This is correct for v3 API - bot_id is used for both bots and agents
                "user_id": f"test_user_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "stream": False,
                "auto_save_history": True,
                "additional_messages": [
                    {
                        "role": "user",
                        "content": message,
                        "content_type": "text"
                    }
                ]
            }
            
            print(f"ğŸ”„ Coze Agent API è°ƒç”¨å°è¯• {attempt + 1}/{max_retries}")
            print(f"ğŸ“ URL: {chat_url}")
            print(f"ğŸ¤– Agent ID: {agent_id}")
            print(f"ğŸ”‘ Token: {access_token[:20]}...")
            
            async with httpx.AsyncClient(timeout=httpx.Timeout(60.0)) as client:
                # Start conversation
                response = await client.post(chat_url, headers=headers, json=payload)
                
                print(f"ğŸ“¡ Response status: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    print(f"âœ… Coze Agent API å“åº”: {result}")
                    
                    if result.get("code") == 0 and "data" in result:
                        conversation_id = result["data"].get("conversation_id")
                        chat_id = result["data"].get("id")
                        status = result["data"].get("status")
                        
                        print(f"ğŸ’¬ å¯¹è¯ID: {conversation_id}")
                        print(f"ğŸ“ èŠå¤©ID: {chat_id}")
                        print(f"ğŸ“Š çŠ¶æ€: {status}")
                        
                        # Use the working GET endpoint for message retrieval
                        messages_url = f"{base_url}/v1/conversation/message/list?conversation_id={conversation_id}&chat_id={chat_id}"
                        
                        # If status is completed, try to get messages immediately
                        if status == "completed":
                            print("ğŸ‰ å¯¹è¯å·²å®Œæˆï¼Œè·å–æ¶ˆæ¯...")
                            messages_response = await client.get(messages_url, headers=headers)
                            
                            if messages_response.status_code == 200:
                                messages_result = messages_response.json()
                                if messages_result.get("code") == 0 and "data" in messages_result:
                                    messages = messages_result["data"]
                                    for msg in messages:
                                        if msg.get("role") == "assistant":
                                            content = msg.get("content", "")
                                            if content and len(content.strip()) > 0:
                                                print(f"âœ… Coze Agent æˆåŠŸå“åº”")
                                                return content
                        
                        elif status == "in_progress":
                            # Enhanced polling for completion - increased for workflow-based agents
                            print("â³ å¯¹è¯å¤„ç†ä¸­ï¼Œå¼€å§‹è½®è¯¢...")
                            max_polls = 40  # Increase to 40 polls for workflow agents
                            poll_interval = 4  # Increase to 4 seconds for better stability
                            
                            for poll in range(max_polls):
                                await asyncio.sleep(poll_interval)
                                
                                # Get messages using the working GET endpoint
                                messages_response = await client.get(messages_url, headers=headers)
                                print(f"ğŸ“‹ è½®è¯¢ {poll + 1}/{max_polls}: {messages_response.status_code}")
                                
                                if messages_response.status_code == 200:
                                    messages_result = messages_response.json()
                                    
                                    if messages_result.get("code") == 0 and "data" in messages_result:
                                        messages = messages_result["data"]
                                        print(f"ğŸ“‹ æ‰¾åˆ° {len(messages)} æ¡æ¶ˆæ¯")
                                        
                                        # Look for assistant response
                                        for msg in messages:
                                            if msg.get("role") == "assistant":
                                                content = msg.get("content", "")
                                                msg_type = msg.get("type", "")
                                                print(f"ğŸ“ åŠ©æ‰‹æ¶ˆæ¯ç±»å‹: {msg_type}, å†…å®¹é•¿åº¦: {len(content)}")
                                                
                                                # Accept any non-empty content from assistant
                                                if content and len(content.strip()) > 0:
                                                    print(f"âœ… Coze Agent è½®è¯¢æˆåŠŸå“åº”")
                                                    return content
                                        
                                        # Check if there are any error messages
                                        for msg in messages:
                                            if msg.get("type") == "error":
                                                error_content = msg.get("content", "")
                                                print(f"âŒ å‘ç°é”™è¯¯æ¶ˆæ¯: {error_content}")
                                                return f"Agentå¤„ç†å¤±è´¥: {error_content}"
                                    else:
                                        error_code = messages_result.get("code", "unknown")
                                        error_msg = messages_result.get("msg", "unknown error")
                                        print(f"âŒ æ¶ˆæ¯åˆ—è¡¨APIé”™è¯¯: code={error_code}, msg={error_msg}")
                                else:
                                    print(f"âŒ æ¶ˆæ¯åˆ—è¡¨API HTTPé”™è¯¯: {messages_response.status_code}")
                                
                                # Check if we should continue polling
                                if poll >= max_polls - 1:
                                    print("â° è½®è¯¢è¶…æ—¶ï¼Œå°è¯•é‡è¯•...")
                                    break
                            
                            # If agent didn't respond, try fallback to Coze Bot API
                            print("ğŸ”„ Agentæœªå“åº”ï¼Œå°è¯•ä½¿ç”¨Coze Bot APIä½œä¸ºå¤‡ç”¨...")
                            fallback_response = await call_coze_api_fallback(message, "7511993619423985674")
                            if fallback_response and "APIè°ƒç”¨å¤±è´¥" not in fallback_response:
                                print("âœ… Coze Bot APIå¤‡ç”¨æˆåŠŸ")
                                return f"[å¤‡ç”¨Botå›å¤] {fallback_response}"
                            else:
                                print("âŒ å¤‡ç”¨APIä¹Ÿå¤±è´¥")
                                return "Agentå’Œå¤‡ç”¨Botéƒ½æœªèƒ½å“åº”ï¼Œè¯·æ£€æŸ¥é…ç½®"
                        
                        elif status == "failed":
                            error_msg = result["data"].get("last_error", {}).get("msg", "æœªçŸ¥é”™è¯¯")
                            print(f"âŒ å¯¹è¯å¤±è´¥: {error_msg}")
                            return f"å¯¹è¯å¤±è´¥: {error_msg}"
                    
                    elif result.get("code") != 0:
                        error_msg = result.get("msg", "APIè°ƒç”¨å¤±è´¥")
                        print(f"âŒ APIé”™è¯¯: {error_msg}")
                        return f"APIé”™è¯¯: {error_msg}"
                    
                    print(f"âš ï¸ Coze Agent API æœªèƒ½è·å–æœ‰æ•ˆå“åº”")
                else:
                    response_text = response.text
                    print(f"âš ï¸ Coze Agent API HTTPé”™è¯¯: {response.status_code}")
                    print(f"å“åº”å†…å®¹: {response_text}")
                    
                    # Parse error response if possible
                    try:
                        error_result = response.json()
                        if "msg" in error_result:
                            return f"APIè°ƒç”¨å¤±è´¥: {error_result['msg']}"
                    except:
                        pass
                    
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # Exponential backoff
                        continue
                        
        except Exception as e:
            print(f"âŒ Coze Agent APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}): {str(e)}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
                continue
    
    return "Coze Agent APIè°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥Agent IDå’ŒAccess Tokenæ˜¯å¦æ­£ç¡®"

# Add new endpoint for user persona extraction
@app.post("/api/extract-user-persona")
async def extract_user_persona_from_document(
    requirement_file: UploadFile = File(None),
    requirement_text: str = Form(None)
):
    """
    Extract user persona and context from requirement document using DeepSeek
    Uses improved document processing approach with temporary files
    """
    try:
        print("ğŸ”============================================================ğŸ”")
        print("   ç”¨æˆ·ç”»åƒæå–æœåŠ¡ (DeepSeekæ™ºèƒ½åˆ†æ) - ä¼˜åŒ–ç‰ˆæœ¬")
        print("ğŸ”============================================================ğŸ”")
        
        # Extract requirement text
        requirement_content = ""
        
        if requirement_file and requirement_file.filename:
            print(f"ğŸ“„ Processing uploaded file: {requirement_file.filename}")
            
            # Use improved document processing
            requirement_content = await process_uploaded_document_improved(requirement_file)
                
        elif requirement_text and requirement_text.strip():
            print("ğŸ“ Using provided text content")
            requirement_content = requirement_text.strip()
            # Clean the text using the same approach
            requirement_content = requirement_content.replace("\r", "").replace("ã€€", "").strip()
            print(f"ğŸ“„ æ–‡æœ¬å†…å®¹é•¿åº¦: {len(requirement_content)} å­—ç¬¦")
        else:
            raise HTTPException(status_code=400, detail="è¯·æä¾›éœ€æ±‚æ–‡æ¡£æ–‡ä»¶æˆ–æ–‡æœ¬å†…å®¹")
        
        # Check if extraction was successful
        if not requirement_content or requirement_content.strip() == "":
            raise HTTPException(status_code=400, detail="æ–‡æ¡£å†…å®¹æå–å¤±è´¥æˆ–ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå†…å®¹")
        
        # Check for extraction errors
        if "è§£æå¤±è´¥" in requirement_content or "å¤„ç†å¤±è´¥" in requirement_content:
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£å¤„ç†é”™è¯¯: {requirement_content}")
        
        # Ensure minimum content length for meaningful analysis
        if len(requirement_content.strip()) < 50:
            raise HTTPException(
                status_code=400, 
                detail=f"æ–‡æ¡£å†…å®¹è¿‡çŸ­({len(requirement_content)}å­—ç¬¦)ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„ç”¨æˆ·ç”»åƒåˆ†æã€‚è¯·æä¾›æ›´è¯¦ç»†çš„éœ€æ±‚æ–‡æ¡£ï¼ˆå»ºè®®è‡³å°‘100å­—ç¬¦ï¼‰"
            )
        
        print(f"ğŸ“„ Document content successfully extracted: {len(requirement_content)} characters")
        print(f"ğŸ“„ Content preview: {requirement_content[:200]}...")
        
        # Call DeepSeek to extract user persona and context
        extraction_result = await extract_user_persona_with_deepseek(requirement_content)
        
        print("âœ… User persona extraction completed successfully")
        return {
            "success": True,
            "extraction_result": extraction_result,
            "document_preview": requirement_content[:300] + "..." if len(requirement_content) > 300 else requirement_content,
            "document_length": len(requirement_content),
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ User persona extraction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”¨æˆ·ç”»åƒæå–å¤±è´¥: {str(e)}")

async def extract_user_persona_with_deepseek(requirement_content: str) -> Dict[str, Any]:
    """
    Use DeepSeek to extract user persona, context, and role information from requirement document
    """
    
    extraction_prompt = f"""
è¯·ä»”ç»†åˆ†æä»¥ä¸‹éœ€æ±‚æ–‡æ¡£ï¼Œæå–ç”¨æˆ·ç”»åƒä¿¡æ¯ã€‚è¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚

éœ€æ±‚æ–‡æ¡£å†…å®¹ï¼š
{requirement_content}

è¯·è¿”å›ä»¥ä¸‹æ ¼å¼çš„JSONï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•é¢å¤–æ–‡å­—ï¼‰ï¼š

{{
    "user_persona": {{
        "role": "å…·ä½“ç”¨æˆ·è§’è‰²ï¼Œå¦‚ï¼šé“¶è¡Œå®¢æœä»£è¡¨ã€ç°åœºç›‘ç†å·¥ç¨‹å¸ˆç­‰",
        "experience_level": "ç»éªŒæ°´å¹³æè¿°ï¼Œå¦‚ï¼š2-8å¹´å®¢æœç»éªŒã€5å¹´å·¥ç¨‹ç»éªŒç­‰", 
        "expertise_areas": ["ä¸“ä¸šé¢†åŸŸ1", "ä¸“ä¸šé¢†åŸŸ2"],
        "communication_style": "æ²Ÿé€šé£æ ¼ï¼Œå¦‚ï¼šä¹ æƒ¯ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ã€åå¥½ç®€æ´æ˜äº†ç­‰",
        "work_environment": "å·¥ä½œç¯å¢ƒæè¿°ï¼Œå¦‚ï¼šå‘¼å«ä¸­å¿ƒã€å»ºç­‘å·¥åœ°ç°åœºç­‰"
    }},
    "usage_context": {{
        "primary_scenarios": ["ä¸»è¦ä½¿ç”¨åœºæ™¯1", "ä¸»è¦ä½¿ç”¨åœºæ™¯2"],
        "business_domain": "ä¸šåŠ¡é¢†åŸŸï¼Œå¦‚ï¼šé“¶è¡Œå®¢æœã€å·¥ç¨‹ç›‘ç†ç­‰",
        "interaction_goals": ["ç”¨æˆ·ç›®æ ‡1", "ç”¨æˆ·ç›®æ ‡2"],
        "pain_points": ["ç—›ç‚¹é—®é¢˜1", "ç—›ç‚¹é—®é¢˜2"]
    }},
    "ai_role_simulation": {{
        "simulated_user_type": "æ¨¡æ‹Ÿç”¨æˆ·ç±»å‹çš„è¯¦ç»†æè¿°",
        "conversation_approach": "å¯¹è¯æ–¹å¼ï¼Œå¦‚ï¼šå¿«é€Ÿæé—®ã€è¯¦ç»†å’¨è¯¢ç­‰", 
        "language_characteristics": "è¯­è¨€ç‰¹ç‚¹ï¼Œå¦‚ï¼šä½¿ç”¨ä¸“ä¸šæœ¯è¯­ã€å£è¯­åŒ–è¡¨è¾¾ç­‰",
        "typical_questions": ["å…¸å‹é—®é¢˜1", "å…¸å‹é—®é¢˜2", "å…¸å‹é—®é¢˜3"]
    }},
    "extracted_requirements": {{
        "core_functions": ["æ ¸å¿ƒåŠŸèƒ½éœ€æ±‚1", "æ ¸å¿ƒåŠŸèƒ½éœ€æ±‚2"],
        "quality_expectations": ["è´¨é‡æœŸæœ›1", "è´¨é‡æœŸæœ›2"],
        "interaction_preferences": ["äº¤äº’åå¥½1", "äº¤äº’åå¥½2"]
    }}
}}

é‡è¦ï¼šåªè¿”å›JSONå†…å®¹ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–å…¶ä»–æ–‡å­—ã€‚"""

    try:
        print("ğŸ§  Calling DeepSeek API for user persona extraction...")
        
        response = await call_deepseek_api(extraction_prompt)
        print(f"ğŸ“ DeepSeek raw response: {response[:200]}...")
        
        # Clean the response - remove any markdown formatting or extra text
        cleaned_response = response.strip()
        
        # Remove markdown code blocks if present
        if cleaned_response.startswith('```'):
            lines = cleaned_response.split('\n')
            cleaned_response = '\n'.join(lines[1:-1]) if len(lines) > 2 else cleaned_response
        
        # Remove any text before the first { and after the last }
        start_idx = cleaned_response.find('{')
        end_idx = cleaned_response.rfind('}')
        
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            json_str = cleaned_response[start_idx:end_idx+1]
            
            try:
                extraction_result = json.loads(json_str)
                print("âœ… Successfully parsed extraction result from DeepSeek")
                
                # Validate the structure
                required_keys = ['user_persona', 'usage_context', 'ai_role_simulation', 'extracted_requirements']
                if all(key in extraction_result for key in required_keys):
                    return extraction_result
                else:
                    print("âš ï¸ JSON structure incomplete, using fallback")
                    
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON parsing failed: {e}, trying alternative parsing...")
        
        # If JSON parsing fails, try to extract specific information from the response
        print("ğŸ“ Creating enhanced structured response from text...")
        return create_enhanced_extraction_result(response, requirement_content)
        
    except Exception as e:
        print(f"âŒ DeepSeek extraction error: {e}")
        # Return a basic fallback result
        return create_basic_fallback_result(requirement_content)

def create_enhanced_extraction_result(response: str, requirement_content: str) -> Dict[str, Any]:
    """
    Create an enhanced structured result when JSON parsing fails but we have DeepSeek response
    """
    # Try to extract information from both the response and the original content
    lines = response.split('\n') + requirement_content.split('\n')
    
    # Extract role information
    role = extract_role_from_content(requirement_content) or extract_info_from_lines(lines, ["è§’è‰²", "ç”¨æˆ·", "èº«ä»½", "ä»£è¡¨"]) or "ä¸“ä¸šç”¨æˆ·"
    
    # Extract experience level
    experience = extract_experience_from_content(requirement_content) or extract_info_from_lines(lines, ["ç»éªŒ", "å¹´é™", "æ°´å¹³"]) or "æœ‰ç»éªŒç”¨æˆ·"
    
    # Extract work environment
    work_env = extract_work_environment_from_content(requirement_content) or extract_info_from_lines(lines, ["ç¯å¢ƒ", "ç°åœº", "åœ°ç‚¹", "ä¸­å¿ƒ"]) or "ä¸“ä¸šå·¥ä½œç¯å¢ƒ"
    
    # Extract business domain
    business_domain = extract_business_domain_from_content(requirement_content) or "ä¸“ä¸šæœåŠ¡"
    
    return {
        "user_persona": {
            "role": role,
            "experience_level": experience,
            "expertise_areas": extract_expertise_areas_from_content(requirement_content),
            "communication_style": extract_communication_style_from_content(requirement_content),
            "work_environment": work_env
        },
        "usage_context": {
            "primary_scenarios": extract_scenarios_from_content(requirement_content),
            "business_domain": business_domain,
            "interaction_goals": extract_goals_from_content(requirement_content),
            "pain_points": extract_pain_points_from_content(requirement_content)
        },
        "ai_role_simulation": {
            "simulated_user_type": f"åŸºäº{business_domain}é¢†åŸŸçš„{role}",
            "conversation_approach": "ç»“åˆå®é™…å·¥ä½œåœºæ™¯çš„ä¸“ä¸šæé—®",
            "language_characteristics": "ä¸“ä¸šæœ¯è¯­ä¸å®é™…éœ€æ±‚ç›¸ç»“åˆ",
            "typical_questions": generate_typical_questions_from_content(requirement_content)
        },
        "extracted_requirements": {
            "core_functions": extract_requirements_from_content(requirement_content),
            "quality_expectations": extract_quality_expectations_from_content(requirement_content),
            "interaction_preferences": extract_interaction_preferences_from_content(requirement_content)
        }
    }

def extract_role_from_content(content: str) -> Optional[str]:
    """Extract user role from content"""
    role_patterns = [
        r'ç”¨æˆ·ç¾¤ä½“[ï¼š:]\s*([^\n]+)',
        r'ä¸»è¦ç”¨æˆ·[ï¼š:]\s*([^\n]+)',
        r'ç›®æ ‡ç”¨æˆ·[ï¼š:]\s*([^\n]+)',
        r'ç”¨æˆ·è§’è‰²[ï¼š:]\s*([^\n]+)'
    ]
    
    for pattern in role_patterns:
        match = re.search(pattern, content)
        if match:
            return match.group(1).strip()
    
    # Look for specific role mentions
    if "å®¢æœ" in content:
        return "å®¢æœä»£è¡¨"
    elif "ç›‘ç†" in content:
        return "ç°åœºç›‘ç†å·¥ç¨‹å¸ˆ"
    elif "å·¥ç¨‹å¸ˆ" in content:
        return "å·¥ç¨‹å¸ˆ"
    elif "æŠ€æœ¯" in content:
        return "æŠ€æœ¯äººå‘˜"
    
    return None

def extract_experience_from_content(content: str) -> Optional[str]:
    """Extract experience level from content"""
    exp_patterns = [
        r'(\d+[-~]\d+å¹´[^ï¼Œã€‚\n]*ç»éªŒ)',
        r'(å·¥ä½œç»éªŒ[ï¼š:][^ï¼Œã€‚\n]+)',
        r'(ç»éªŒæ°´å¹³[ï¼š:][^ï¼Œã€‚\n]+)'
    ]
    
    for pattern in exp_patterns:
        match = re.search(pattern, content)
        if match:
            return match.group(1).strip()
    
    return None

def extract_work_environment_from_content(content: str) -> Optional[str]:
    """Extract work environment from content"""
    env_patterns = [
        r'å·¥ä½œç¯å¢ƒ[ï¼š:]\s*([^ï¼Œã€‚\n]+)',
        r'ç¯å¢ƒ[ï¼š:]\s*([^ï¼Œã€‚\n]+)'
    ]
    
    for pattern in env_patterns:
        match = re.search(pattern, content)
        if match:
            return match.group(1).strip()
    
    # Look for specific environment mentions
    if "å‘¼å«ä¸­å¿ƒ" in content:
        return "å‘¼å«ä¸­å¿ƒ"
    elif "ç°åœº" in content or "å·¥åœ°" in content:
        return "æ–½å·¥ç°åœº"
    elif "åŠå…¬å®¤" in content:
        return "åŠå…¬å®¤ç¯å¢ƒ"
    
    return None

def extract_business_domain_from_content(content: str) -> str:
    """Extract business domain from content"""
    if "é“¶è¡Œ" in content or "é‡‘è" in content:
        return "é“¶è¡Œé‡‘èæœåŠ¡"
    elif "å»ºç­‘" in content or "å·¥ç¨‹" in content:
        return "å»ºç­‘å·¥ç¨‹"
    elif "å®¢æœ" in content:
        return "å®¢æˆ·æœåŠ¡"
    elif "æŠ€æœ¯" in content:
        return "æŠ€æœ¯æ”¯æŒ"
    else:
        return "ä¸“ä¸šæœåŠ¡"

def extract_expertise_areas_from_content(content: str) -> List[str]:
    """Extract expertise areas from content"""
    areas = []
    
    if "å®¢æœ" in content:
        areas.extend(["å®¢æˆ·æœåŠ¡", "ä¸šåŠ¡å’¨è¯¢"])
    if "é‡‘è" in content or "é“¶è¡Œ" in content:
        areas.extend(["é‡‘èäº§å“", "é“¶è¡Œä¸šåŠ¡"])
    if "å·¥ç¨‹" in content or "å»ºç­‘" in content:
        areas.extend(["å·¥ç¨‹æŠ€æœ¯", "è´¨é‡ç®¡ç†"])
    if "æŠ€æœ¯" in content:
        areas.extend(["æŠ€æœ¯æ”¯æŒ", "ç³»ç»Ÿæ“ä½œ"])
    
    return areas if areas else ["ä¸“ä¸šæŠ€æœ¯", "è¡Œä¸šçŸ¥è¯†"]

def extract_communication_style_from_content(content: str) -> str:
    """Extract communication style from content"""
    if "ä¸“ä¸šæœ¯è¯­" in content:
        return "ä¹ æƒ¯ä½¿ç”¨ä¸“ä¸šæœ¯è¯­"
    elif "ç®€æ´" in content or "å¿«é€Ÿ" in content:
        return "åå¥½ç®€æ´æ˜äº†çš„æ²Ÿé€š"
    elif "ç»“æ„åŒ–" in content or "æ¡ç†" in content:
        return "åå¥½ç»“æ„åŒ–ã€æ¡ç†æ¸…æ™°çš„å›ç­”"
    else:
        return "ä¸“ä¸šæœ¯è¯­ä¸é€šä¿—è§£é‡Šç»“åˆ"

def extract_scenarios_from_content(content: str) -> List[str]:
    """Extract primary scenarios from content"""
    scenarios = []
    
    # Look for numbered lists or bullet points
    scenario_patterns = [
        r'\d+\.\s*([^\n]+)',
        r'-\s*([^\n]+åœºæ™¯[^\n]*)',
        r'â€¢\s*([^\n]+)'
    ]
    
    for pattern in scenario_patterns:
        matches = re.findall(pattern, content)
        scenarios.extend([match.strip() for match in matches[:3]])  # Limit to 3
    
    if not scenarios:
        if "æŸ¥è¯¢" in content:
            scenarios.append("ä¿¡æ¯æŸ¥è¯¢")
        if "å’¨è¯¢" in content:
            scenarios.append("ä¸šåŠ¡å’¨è¯¢")
        if "é—®é¢˜è§£å†³" in content or "å¤„ç†" in content:
            scenarios.append("é—®é¢˜è§£å†³")
    
    return scenarios if scenarios else ["æŠ€æœ¯å’¨è¯¢", "é—®é¢˜è§£å†³"]

def extract_goals_from_content(content: str) -> List[str]:
    """Extract interaction goals from content"""
    goals = []
    
    if "å‡†ç¡®" in content:
        goals.append("è·å–å‡†ç¡®ä¿¡æ¯")
    if "å¿«é€Ÿ" in content or "è¿…é€Ÿ" in content:
        goals.append("å¿«é€Ÿå“åº”")
    if "è§£å†³" in content:
        goals.append("è§£å†³å®é™…é—®é¢˜")
    if "æ•ˆç‡" in content:
        goals.append("æé«˜å·¥ä½œæ•ˆç‡")
    
    return goals if goals else ["è·å–å‡†ç¡®ä¿¡æ¯", "è§£å†³å®é™…é—®é¢˜"]

def extract_pain_points_from_content(content: str) -> List[str]:
    """Extract pain points from content"""
    pain_points = []
    
    if "å“åº”æ—¶é—´" in content or "æ…¢" in content:
        pain_points.append("å“åº”é€Ÿåº¦æ…¢")
    if "å‡†ç¡®" in content:
        pain_points.append("ä¿¡æ¯ä¸å¤Ÿå‡†ç¡®")
    if "å‹åŠ›" in content:
        pain_points.append("å·¥ä½œå‹åŠ›å¤§")
    if "å¤æ‚" in content:
        pain_points.append("æ“ä½œè¿‡äºå¤æ‚")
    
    return pain_points if pain_points else ["ä¿¡æ¯è·å–å›°éš¾", "å“åº”ä¸åŠæ—¶"]

def extract_quality_expectations_from_content(content: str) -> List[str]:
    """Extract quality expectations from content"""
    expectations = []
    
    if "å‡†ç¡®ç‡" in content or "95%" in content:
        expectations.append("é«˜å‡†ç¡®ç‡(95%ä»¥ä¸Š)")
    if "å“åº”æ—¶é—´" in content or "3ç§’" in content:
        expectations.append("å¿«é€Ÿå“åº”(3ç§’å†…)")
    if "åˆè§„" in content:
        expectations.append("ç¬¦åˆåˆè§„è¦æ±‚")
    
    return expectations if expectations else ["é«˜å‡†ç¡®æ€§", "å¿«é€Ÿå“åº”", "ä¸“ä¸šå¯é "]

def extract_interaction_preferences_from_content(content: str) -> List[str]:
    """Extract interaction preferences from content"""
    preferences = []
    
    if "ç»“æ„åŒ–" in content:
        preferences.append("ç»“æ„åŒ–å›ç­”")
    if "æ¡ç†" in content:
        preferences.append("æ¡ç†æ¸…æ™°")
    if "æ¨¡æ¿" in content:
        preferences.append("æ ‡å‡†åŒ–æ¨¡æ¿")
    if "è¯­éŸ³" in content:
        preferences.append("æ”¯æŒè¯­éŸ³è¾“å…¥")
    
    return preferences if preferences else ["æ¸…æ™°æ˜äº†", "é€»è¾‘æ¸…æ¥š", "å®ç”¨å¯è¡Œ"]

def generate_typical_questions_from_content(content: str) -> List[str]:
    """Generate typical questions based on content analysis"""
    questions = []
    
    if "è´¦æˆ·" in content:
        questions.append("å®¢æˆ·è´¦æˆ·ä½™é¢æ€ä¹ˆæŸ¥è¯¢ï¼Ÿ")
    if "äº§å“" in content:
        questions.append("è¿™ä¸ªé‡‘èäº§å“çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ")
    if "æµç¨‹" in content:
        questions.append("ä¸šåŠ¡åŠç†æµç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ")
    if "å·¥ç¨‹" in content:
        questions.append("è¿™ä¸ªè´¨é‡é—®é¢˜æ€ä¹ˆå¤„ç†ï¼Ÿ")
    if "è§„èŒƒ" in content:
        questions.append("ç›¸å…³è§„èŒƒè¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ")
    
    return questions if questions else ["è¿™ä¸ªé—®é¢˜æ€ä¹ˆè§£å†³ï¼Ÿ", "æœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„ï¼Ÿ", "èƒ½è¯¦ç»†è¯´æ˜ä¸€ä¸‹å—ï¼Ÿ"]

def extract_info_from_lines(lines: List[str], keywords: List[str]) -> Optional[str]:
    """
    Extract information from text lines based on keywords
    """
    for line in lines:
        for keyword in keywords:
            if keyword in line:
                # Try to extract the content after the keyword
                parts = line.split(':', 1)
                if len(parts) > 1:
                    return parts[1].strip()
                # Try to extract content after common separators
                for sep in ['ï¼š', 'æ˜¯', 'ä¸º']:
                    if sep in line:
                        parts = line.split(sep, 1)
                        if len(parts) > 1:
                            return parts[1].strip()
    return None

def extract_requirements_from_content(content: str) -> List[str]:
    """
    Extract core requirements from document content
    """
    requirements = []
    
    # Look for common requirement indicators
    requirement_keywords = ['åŠŸèƒ½', 'éœ€æ±‚', 'è¦æ±‚', 'ç›®æ ‡', 'æœŸæœ›']
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        if any(keyword in line for keyword in requirement_keywords) and len(line) > 10:
            requirements.append(line[:100])  # Limit length
            if len(requirements) >= 3:  # Limit to 3 requirements
                break
    
    if not requirements:
        requirements = ["AIç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½éœ€æ±‚", "ç”¨æˆ·äº¤äº’ä½“éªŒä¼˜åŒ–", "å‡†ç¡®æ€§å’Œå¯é æ€§ä¿éšœ"]
    
    return requirements

def create_basic_fallback_result(requirement_content: str) -> Dict[str, Any]:
    """
    Create a basic fallback result when extraction completely fails
    """
    return {
        "user_persona": {
            "role": "ä¸“ä¸šç”¨æˆ·",
            "experience_level": "ä¸­ç­‰ç»éªŒæ°´å¹³",
            "expertise_areas": ["ä¸“ä¸šæŠ€æœ¯", "è¡Œä¸šçŸ¥è¯†"],
            "communication_style": "ä¸“ä¸šä¸é€šä¿—å¹¶é‡",
            "work_environment": "ä¸“ä¸šå·¥ä½œåœºæ‰€"
        },
        "usage_context": {
            "primary_scenarios": ["æŠ€æœ¯å’¨è¯¢", "é—®é¢˜è§£å†³", "ä¿¡æ¯æŸ¥è¯¢"],
            "business_domain": "ä¸“ä¸šæœåŠ¡",
            "interaction_goals": ["è·å–å‡†ç¡®ä¿¡æ¯", "è§£å†³å®é™…é—®é¢˜", "æé«˜å·¥ä½œæ•ˆç‡"],
            "pain_points": ["ä¿¡æ¯è·å–å›°éš¾", "å›ç­”ä¸å¤Ÿä¸“ä¸š", "å“åº”é€Ÿåº¦æ…¢"]
        },
        "ai_role_simulation": {
            "simulated_user_type": "å…·æœ‰ä¸€å®šä¸“ä¸šèƒŒæ™¯çš„å®é™…ç”¨æˆ·",
            "conversation_approach": "ç»“åˆå®é™…å·¥ä½œåœºæ™¯çš„è‡ªç„¶æé—®",
            "language_characteristics": "ä¸“ä¸šæœ¯è¯­ä¸æ—¥å¸¸è¯­è¨€ç»“åˆ",
            "typical_questions": ["è¿™ä¸ªé—®é¢˜æ€ä¹ˆè§£å†³ï¼Ÿ", "æœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼Ÿ", "èƒ½è¯¦ç»†è¯´æ˜ä¸€ä¸‹å—ï¼Ÿ"]
        },
        "extracted_requirements": {
            "core_functions": ["æ ¸å¿ƒåŠŸèƒ½å®ç°", "ç”¨æˆ·ä½“éªŒä¼˜åŒ–", "ä¸“ä¸šå‡†ç¡®æ€§ä¿éšœ"],
            "quality_expectations": ["é«˜å‡†ç¡®æ€§", "è‰¯å¥½å“åº”é€Ÿåº¦", "ä¸“ä¸šå¯é "],
            "interaction_preferences": ["æ¸…æ™°æ˜“æ‡‚", "é€»è¾‘æ¸…æ¥š", "å®ç”¨å¯è¡Œ"]
        }
    }

async def generate_conversation_scenarios_from_persona(user_persona_info: Dict) -> List[Dict]:
    """
    Generate realistic conversation scenarios based on extracted user persona using DeepSeek
    """
    try:
        persona = user_persona_info.get('user_persona', {})
        usage_context = user_persona_info.get('usage_context', {})
        ai_role = user_persona_info.get('ai_role_simulation', {})
        
        generation_prompt = f"""
åŸºäºä»¥ä¸‹ç”¨æˆ·ç”»åƒä¿¡æ¯ï¼Œè¯·ç”Ÿæˆ3ä¸ªé€¼çœŸçš„å¯¹è¯æµ‹è¯•åœºæ™¯ã€‚æ¯ä¸ªåœºæ™¯åº”è¯¥ï¼š
1. ä½“ç°è¯¥ç”¨æˆ·è§’è‰²çš„å®é™…å·¥ä½œéœ€æ±‚
2. åŒ…å«è¯¥ç”¨æˆ·çš„æ²Ÿé€šé£æ ¼å’Œè¯­è¨€ä¹ æƒ¯
3. è®¾è®¡3-4è½®æ¸è¿›å¼å¯¹è¯ï¼Œä»æ¨¡ç³Šåˆ°å…·ä½“

ç”¨æˆ·ç”»åƒä¿¡æ¯ï¼š
- è§’è‰²ï¼š{persona.get('role', '')}
- ç»éªŒæ°´å¹³ï¼š{persona.get('experience_level', '')}
- å·¥ä½œç¯å¢ƒï¼š{persona.get('work_environment', '')}
- æ²Ÿé€šé£æ ¼ï¼š{persona.get('communication_style', '')}
- ä¸šåŠ¡é¢†åŸŸï¼š{usage_context.get('business_domain', '')}
- ä¸»è¦åœºæ™¯ï¼š{', '.join(usage_context.get('primary_scenarios', []))}
- å…¸å‹é—®é¢˜ï¼š{', '.join(ai_role.get('typical_questions', []))}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼š

[
  {{
    "title": "åœºæ™¯1æ ‡é¢˜",
    "context": "ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿°",
    "user_profile": "ç”¨æˆ·ç”»åƒç®€è¿°",
    "turns": [
      "ç¬¬1è½®ï¼šæ¨¡ç³Šæˆ–ç®€çŸ­çš„é—®é¢˜",
      "ç¬¬2è½®ï¼šè¡¥å……ä¿¡æ¯æˆ–æ¾„æ¸…",
      "ç¬¬3è½®ï¼šè¿›ä¸€æ­¥ç»†èŠ‚",
      "ç¬¬4è½®ï¼šå…·ä½“éœ€æ±‚ï¼ˆå¯é€‰ï¼‰"
    ]
  }},
  {{
    "title": "åœºæ™¯2æ ‡é¢˜",
    "context": "ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿°",
    "user_profile": "ç”¨æˆ·ç”»åƒç®€è¿°",
    "turns": [
      "ç¬¬1è½®ï¼šæ¨¡ç³Šæˆ–ç®€çŸ­çš„é—®é¢˜",
      "ç¬¬2è½®ï¼šè¡¥å……ä¿¡æ¯æˆ–æ¾„æ¸…",
      "ç¬¬3è½®ï¼šè¿›ä¸€æ­¥ç»†èŠ‚"
    ]
  }},
  {{
    "title": "åœºæ™¯3æ ‡é¢˜",
    "context": "ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿°",
    "user_profile": "ç”¨æˆ·ç”»åƒç®€è¿°",
    "turns": [
      "ç¬¬1è½®ï¼šæ¨¡ç³Šæˆ–ç®€çŸ­çš„é—®é¢˜",
      "ç¬¬2è½®ï¼šè¡¥å……ä¿¡æ¯æˆ–æ¾„æ¸…",
      "ç¬¬3è½®ï¼šè¿›ä¸€æ­¥ç»†èŠ‚"
    ]
  }}
]

é‡è¦ï¼šåªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–å…¶ä»–æ–‡å­—ã€‚"""

        print("ğŸ¯ Generating conversation scenarios based on persona...")
        response = await call_deepseek_api(generation_prompt)
        
        # Clean and parse the response
        cleaned_response = response.strip()
        
        # Remove markdown code blocks if present
        if cleaned_response.startswith('```'):
            lines = cleaned_response.split('\n')
            cleaned_response = '\n'.join(lines[1:-1]) if len(lines) > 2 else cleaned_response
        
        # Extract JSON array
        start_idx = cleaned_response.find('[')
        end_idx = cleaned_response.rfind(']')
        
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            json_str = cleaned_response[start_idx:end_idx+1]
            
            try:
                scenarios = json.loads(json_str)
                if isinstance(scenarios, list) and len(scenarios) > 0:
                    print(f"âœ… Generated {len(scenarios)} conversation scenarios")
                    return scenarios
                    
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON parsing failed: {e}, using fallback generation...")
        
        # Fallback: generate scenarios based on persona information
        return generate_fallback_scenarios_from_persona(user_persona_info)
        
    except Exception as e:
        print(f"âŒ Scenario generation error: {e}")
        # Return basic fallback scenarios
        return generate_fallback_scenarios_from_persona(user_persona_info)

def generate_fallback_scenarios_from_persona(user_persona_info: Dict) -> List[Dict]:
    """
    Generate fallback scenarios when DeepSeek API fails
    """
    persona = user_persona_info.get('user_persona', {})
    usage_context = user_persona_info.get('usage_context', {})
    
    role = persona.get('role', 'ä¸“ä¸šç”¨æˆ·')
    business_domain = usage_context.get('business_domain', 'ä¸“ä¸šæœåŠ¡')
    primary_scenarios = usage_context.get('primary_scenarios', [])
    
    # Generate scenarios based on role and domain
    if 'å®¢æœ' in role:
        return [
            {
                "title": "å®¢æˆ·è´¦æˆ·æŸ¥è¯¢",
                "context": f"{business_domain}å®¢æœåœºæ™¯",
                "user_profile": f"{role}ï¼Œ{persona.get('experience_level', 'æœ‰ç»éªŒ')}",
                "turns": [
                    "å®¢æˆ·é—®è´¦æˆ·ä½™é¢",
                    "éœ€è¦éªŒè¯èº«ä»½ä¿¡æ¯",
                    "å…·ä½“æ˜¯å“ªä¸ªè´¦æˆ·"
                ]
            },
            {
                "title": "äº§å“å’¨è¯¢",
                "context": f"{business_domain}äº§å“å’¨è¯¢",
                "user_profile": f"{role}ï¼Œ{persona.get('communication_style', 'ä¸“ä¸šæ²Ÿé€š')}",
                "turns": [
                    "å®¢æˆ·æƒ³äº†è§£æ–°äº§å“",
                    "è¯¢é—®å…·ä½“åŠŸèƒ½",
                    "å’¨è¯¢è´¹ç”¨å’Œæ¡ä»¶"
                ]
            },
            {
                "title": "ä¸šåŠ¡åŠç†",
                "context": f"{business_domain}ä¸šåŠ¡å¤„ç†",
                "user_profile": f"{role}ï¼Œ{persona.get('work_environment', 'å®¢æœç¯å¢ƒ')}",
                "turns": [
                    "å®¢æˆ·è¦åŠç†ä¸šåŠ¡",
                    "ç¡®è®¤å…·ä½“ä¸šåŠ¡ç±»å‹",
                    "è¯¢é—®æ‰€éœ€ææ–™"
                ]
            }
        ]
    elif 'å·¥ç¨‹å¸ˆ' in role or 'ç›‘ç†' in role:
        return [
            {
                "title": "ç°åœºè´¨é‡é—®é¢˜",
                "context": f"{persona.get('work_environment', 'å·¥ç¨‹ç°åœº')}",
                "user_profile": f"{role}ï¼Œ{persona.get('experience_level', 'æœ‰ç»éªŒ')}",
                "turns": [
                    "å‘ç°è´¨é‡é—®é¢˜",
                    "æè¿°å…·ä½“ä½ç½®",
                    "è¯¢é—®å¤„ç†æ–¹æ³•"
                ]
            },
            {
                "title": "è§„èŒƒæ ‡å‡†æŸ¥è¯¢",
                "context": f"{business_domain}æŠ€æœ¯æ”¯æŒ",
                "user_profile": f"{role}ï¼Œ{persona.get('communication_style', 'ä¸“ä¸šæœ¯è¯­')}",
                "turns": [
                    "éœ€è¦æŸ¥è¯¢ç›¸å…³è§„èŒƒ",
                    "æ˜ç¡®å…·ä½“æ ‡å‡†æ¡æ–‡",
                    "ç¡®è®¤é€‚ç”¨æ¡ä»¶"
                ]
            },
            {
                "title": "å·¥è‰ºæµç¨‹å’¨è¯¢",
                "context": f"{business_domain}ç°åœºä½œä¸š",
                "user_profile": f"{role}ï¼Œ{persona.get('work_environment', 'æ–½å·¥ç°åœº')}",
                "turns": [
                    "å’¨è¯¢æ–½å·¥å·¥è‰º",
                    "è¯¢é—®å…·ä½“æ­¥éª¤",
                    "ç¡®è®¤è´¨é‡è¦æ±‚"
                ]
            }
        ]
    else:
        # Generic scenarios
        scenarios_list = []
        for i, scenario in enumerate(primary_scenarios[:3], 1):
            scenarios_list.append({
                "title": f"{scenario}å’¨è¯¢",
                "context": f"{business_domain}",
                "user_profile": f"{role}ï¼Œ{persona.get('experience_level', 'ä¸­ç­‰ç»éªŒ')}",
                "turns": [
                    f"å…³äº{scenario}çš„é—®é¢˜",
                    "éœ€è¦æ›´è¯¦ç»†çš„ä¿¡æ¯",
                    "ç¡®è®¤å…·ä½“æ“ä½œæ–¹æ³•"
                ]
            })
        
        if not scenarios_list:
            scenarios_list = [
                {
                    "title": "ä¸“ä¸šå’¨è¯¢",
                    "context": business_domain,
                    "user_profile": f"{role}ï¼Œä¸“ä¸šå·¥ä½œåœºæ™¯",
                    "turns": [
                        "æœ‰ä¸ªé—®é¢˜éœ€è¦å’¨è¯¢",
                        "å…·ä½“æƒ…å†µæ˜¯è¿™æ ·çš„",
                        "åº”è¯¥æ€ä¹ˆå¤„ç†"
                    ]
                }
            ]
        
        return scenarios_list

# Remove the old fallback scenario functions and replace with simple scenario generation
async def generate_dynamic_conversation_scenarios(user_persona_info: Dict) -> List[Dict]:
    """
    Generate 2 realistic conversation scenarios with strict timeout - no fallbacks
    """
    try:
        persona = user_persona_info.get('user_persona', {})
        usage_context = user_persona_info.get('usage_context', {})
        
        # Generate scenario topics based on persona
        topic_generation_prompt = f"""
åŸºäºç”¨æˆ·ç”»åƒï¼Œç”Ÿæˆ2ä¸ªçœŸå®çš„å¯¹è¯åœºæ™¯ä¸»é¢˜ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼š

ç”¨æˆ·è§’è‰²ï¼š{persona.get('role', '')}
å·¥ä½œç¯å¢ƒï¼š{persona.get('work_environment', '')}
ä¸šåŠ¡é¢†åŸŸï¼š{usage_context.get('business_domain', '')}
ä¸»è¦åœºæ™¯ï¼š{', '.join(usage_context.get('primary_scenarios', []))}

JSONæ ¼å¼ï¼š
[
  {{
    "title": "åœºæ™¯1æ ‡é¢˜",
    "context": "ä¸šåŠ¡ä¸Šä¸‹æ–‡",
    "scenario_type": "å…·ä½“åœºæ™¯ç±»å‹"
  }},
  {{
    "title": "åœºæ™¯2æ ‡é¢˜", 
    "context": "ä¸šåŠ¡ä¸Šä¸‹æ–‡",
    "scenario_type": "å…·ä½“åœºæ™¯ç±»å‹"
  }}
]

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

        print("ğŸ¯ Generating scenario topics based on persona...")
        response = await call_deepseek_with_strict_timeout(topic_generation_prompt)
        
        # Parse scenario topics
        if response:
            cleaned_response = response.strip()
            if cleaned_response.startswith('```'):
                lines = cleaned_response.split('\n')
                cleaned_response = '\n'.join(lines[1:-1]) if len(lines) > 2 else cleaned_response
            
            start_idx = cleaned_response.find('[')
            end_idx = cleaned_response.rfind(']')
            
            if start_idx != -1 and end_idx != -1:
                json_str = cleaned_response[start_idx:end_idx+1]
                parsed_scenarios = json.loads(json_str)
                if isinstance(parsed_scenarios, list) and len(parsed_scenarios) >= 2:
                    scenarios = parsed_scenarios[:2]  # Take first 2 scenarios
                    print(f"âœ… Generated {len(scenarios)} scenarios from DeepSeek")
                    return scenarios
        
        # Simple hardcoded scenarios if API fails - no complex fallbacks
        role = persona.get('role', 'ç”¨æˆ·')
        business_domain = usage_context.get('business_domain', 'ä¸“ä¸šæœåŠ¡')
        
        if 'å·¥ç¨‹å¸ˆ' in role or 'ç›‘ç†' in role:
            return [
                {"title": "é’¢ç­‹éšè”½å·¥ç¨‹éªŒæ”¶", "context": f"{business_domain}ç°åœºéªŒæ”¶", "scenario_type": "æŠ€æœ¯å’¨è¯¢"},
                {"title": "æ··å‡åœŸæµ‡ç­‘æ—ç«™ç›‘ç£", "context": f"{business_domain}ç°åœºç›‘ç£", "scenario_type": "è§„èŒƒæŸ¥è¯¢"}
            ]
        elif 'å®¢æœ' in role:
            return [
                {"title": "å®¢æˆ·è´¦æˆ·æŸ¥è¯¢", "context": f"{business_domain}å®¢æœåœºæ™¯", "scenario_type": "ä¸šåŠ¡å’¨è¯¢"},
                {"title": "äº§å“åŠŸèƒ½å’¨è¯¢", "context": f"{business_domain}äº§å“æ”¯æŒ", "scenario_type": "äº§å“å’¨è¯¢"}
            ]
        else:
            return [
                {"title": "ä¸“ä¸šæŠ€æœ¯å’¨è¯¢", "context": business_domain, "scenario_type": "æŠ€æœ¯æ”¯æŒ"},
                {"title": "æ“ä½œæŒ‡å¯¼é—®é¢˜", "context": f"{business_domain}æ“ä½œåœºæ™¯", "scenario_type": "æ“ä½œæŒ‡å¯¼"}
            ]
        
    except Exception as e:
        print(f"âŒ Scenario generation failed: {str(e)}")
        raise Exception(f"åœºæ™¯ç”Ÿæˆå¤±è´¥: {str(e)}")

# Update the main conversation function to use new approach
async def conduct_dynamic_multi_scenario_evaluation(
    api_config: APIConfig, 
    user_persona_info: Dict, 
    requirement_context: str = ""
) -> List[Dict]:
    """
    Conduct dynamic multi-scenario evaluation with strict timeouts and no fallbacks
    """
    print("ğŸš€ å¼€å§‹åŠ¨æ€å¤šåœºæ™¯å¯¹è¯è¯„ä¼°...")
    
    # Generate 2 scenarios with strict timeout
    scenarios = await generate_dynamic_conversation_scenarios(user_persona_info)
    
    evaluation_results = []
    
    # Dimension names mapping for display
    dimension_names = {
        "fuzzy_understanding": "æ¨¡ç³Šç†è§£",
        "answer_correctness": "å›ç­”å‡†ç¡®æ€§", 
        "persona_alignment": "ç”¨æˆ·åŒ¹é…",
        "goal_alignment": "ç›®æ ‡å¯¹é½"
    }
    
    # Process scenarios sequentially for better error handling
    for i, scenario_info in enumerate(scenarios, 1):
        try:
            print(f"ğŸ“‹ åœºæ™¯ {i}/2: {scenario_info.get('title', 'æœªå‘½ååœºæ™¯')}")
            
            # Conduct true dynamic conversation for this scenario
            conversation_history = await conduct_true_dynamic_conversation(
                api_config, scenario_info, user_persona_info
            )
            
            if conversation_history:
                # Evaluate conversation with enhanced detailed explanations
                try:
                    evaluation_scores, detailed_explanations, scenario_score = await evaluate_conversation_with_deepseek(
                        conversation_history, scenario_info, requirement_context, user_persona_info
                    )
                except Exception as e:
                    print(f"âŒ åœºæ™¯ {i} è¯„ä¼°å¤±è´¥: {str(e)}")
                    continue

                result = {
                    "scenario_title": scenario_info.get('title', f'åœºæ™¯ {i}'),
                    "scenario_context": scenario_info.get('context', 'ä¸“ä¸šå·¥ä½œç¯å¢ƒ'),
                    "scenario_persona": f"{user_persona_info.get('user_persona', {}).get('role', 'ä¸“ä¸šç”¨æˆ·')} | {user_persona_info.get('usage_context', {}).get('business_domain', 'ä¸“ä¸šæœåŠ¡')}",
                    "conversation_history": conversation_history,
                    "evaluation_scores": evaluation_scores,
                    "detailed_explanations": detailed_explanations,  # Add detailed explanations
                    "scenario_score": scenario_score,
                    "total_turns": len(conversation_history),
                    "persona_context_display": {
                        "user_role": user_persona_info.get('user_persona', {}).get('role', 'ä¸“ä¸šç”¨æˆ·'),
                        "business_domain": user_persona_info.get('usage_context', {}).get('business_domain', 'ä¸“ä¸šæœåŠ¡'),
                        "experience_level": user_persona_info.get('user_persona', {}).get('experience_level', 'ä¸­ç­‰ç»éªŒ'),
                        "communication_style": user_persona_info.get('user_persona', {}).get('communication_style', 'ä¸“ä¸šæ²Ÿé€š')
                    },
                    # Frontend display format
                    "scenario": {
                        "title": scenario_info.get('title', f'åœºæ™¯ {i}'),
                        "context": f"{user_persona_info.get('usage_context', {}).get('business_domain', 'ä¸“ä¸šæœåŠ¡')} - {scenario_info.get('context', 'ä¸“ä¸šå·¥ä½œç¯å¢ƒ')}",
                        "user_profile": f"{user_persona_info.get('user_persona', {}).get('role', 'ç”¨æˆ·')}ï¼Œ{user_persona_info.get('user_persona', {}).get('experience_level', 'ä¸­ç­‰ç»éªŒ')}ï¼Œ{user_persona_info.get('user_persona', {}).get('communication_style', 'ä¸“ä¸šæ²Ÿé€š')}"
                    },
                    "evaluation_scores_with_explanations": {
                        dimension: {
                            "score": score,
                            "explanation": detailed_explanations.get(dimension, {}).get("detailed_analysis", "æœªæä¾›è¯¦ç»†åˆ†æ"),
                            "formatted_score": f"{dimension_names.get(dimension, dimension)}: {score}/5"
                        }
                        for dimension, score in evaluation_scores.items()
                    }
                }
                
                evaluation_results.append(result)
                print(f"ğŸ¯ åœºæ™¯ {i} å¾—åˆ†: {scenario_score:.2f}/5.0")
            else:
                print(f"âŒ åœºæ™¯ {i} å¯¹è¯å¤±è´¥")
                
        except Exception as e:
            print(f"âŒ åœºæ™¯ {i} è¯„ä¼°å¤±è´¥: {str(e)}")
            continue
    
    if not evaluation_results:
        raise Exception("æ‰€æœ‰åœºæ™¯è¯„ä¼°å‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥AI Agenté…ç½®å’Œç½‘ç»œè¿æ¥")
    
    return evaluation_results

async def evaluate_conversation_with_deepseek(
    conversation_history: List[Dict], 
    scenario_info: Dict, 
    requirement_context: str,
    user_persona_info: Dict
) -> tuple:
    """
    Enhanced DeepSeek evaluation with detailed explanations for each rating criteria
    """
    try:
        print("ğŸ§  å¼€å§‹DeepSeekæ™ºèƒ½è¯„ä¼° (åŸºäºæå–çš„ç”¨æˆ·ç”»åƒ)...")
        
        persona = user_persona_info.get('user_persona', {})
        context = user_persona_info.get('usage_context', {})
        requirements = user_persona_info.get('extracted_requirements', {})
        
        # Build comprehensive context for detailed evaluation
        context_section = f"""
=== ç”¨æˆ·ç”»åƒåŸºå‡† ===
ç”¨æˆ·è§’è‰²: {persona.get('role', 'ä¸“ä¸šç”¨æˆ·')}
ç»éªŒæ°´å¹³: {persona.get('experience_level', 'ä¸­ç­‰ç»éªŒ')}
å·¥ä½œç¯å¢ƒ: {persona.get('work_environment', 'ä¸“ä¸šç¯å¢ƒ')}
ä¸šåŠ¡é¢†åŸŸ: {context.get('business_domain', 'ä¸“ä¸šæœåŠ¡')}
æ²Ÿé€šé£æ ¼: {persona.get('communication_style', 'ä¸“ä¸šæ²Ÿé€š')}
ä¸“ä¸šé¢†åŸŸ: {', '.join(persona.get('expertise_areas', []))}

=== åœºæ™¯èƒŒæ™¯ ===
åœºæ™¯æ ‡é¢˜: {scenario_info.get('title', 'æœªçŸ¥åœºæ™¯')}
åœºæ™¯æè¿°: {scenario_info.get('context', 'ä¸“ä¸šå¯¹è¯åœºæ™¯')}

=== å®Œæ•´å¯¹è¯è®°å½• ===
"""
        
        # Add conversation history to context
        for i, turn in enumerate(conversation_history, 1):
            context_section += f"ç¬¬{i}è½® - {turn.get('role', 'ç”¨æˆ·')}: {turn.get('content', '')[:100]}...\n"
            
        # Enhanced evaluation prompts with detailed analysis requirements
        evaluation_prompts = {
            "fuzzy_understanding": f"""
{context_section}

è¯·è¯¦ç»†åˆ†æä¸Šè¿°å®Œæ•´å¯¹è¯ä¸­AIçš„æ¨¡ç³Šç†è§£ä¸è¿½é—®èƒ½åŠ›ï¼Œå¹¶ç»™å‡º1-5åˆ†è¯„åˆ†ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
5åˆ†ï¼šå®Œå…¨ç†è§£æ¨¡ç³Šé—®é¢˜ï¼Œä¸»åŠ¨è¿½é—®å…³é”®ç»†èŠ‚ï¼Œå¼•å¯¼ç”¨æˆ·æä¾›å¿…è¦ä¿¡æ¯
4åˆ†ï¼šåŸºæœ¬ç†è§£æ¨¡ç³Šé—®é¢˜ï¼Œæœ‰ä¸€å®šè¿½é—®èƒ½åŠ›
3åˆ†ï¼šèƒ½å¤„ç†éƒ¨åˆ†æ¨¡ç³Šé—®é¢˜ï¼Œè¿½é—®ä¸å¤Ÿæ·±å…¥
2åˆ†ï¼šå¯¹æ¨¡ç³Šé—®é¢˜ç†è§£æœ‰é™ï¼Œå¾ˆå°‘ä¸»åŠ¨è¿½é—®
1åˆ†ï¼šæ— æ³•å¤„ç†æ¨¡ç³Šé—®é¢˜ï¼Œç¼ºä¹è¿½é—®èƒ½åŠ›

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
è¯„åˆ†ï¼šX/5
è¯¦ç»†åˆ†æï¼š
1. æ¨¡ç³Šç†è§£è¡¨ç°ï¼š[åˆ†æAIå¦‚ä½•ç†è§£ç”¨æˆ·çš„æ¨¡ç³Šæˆ–ä¸å®Œæ•´é—®é¢˜]
2. è¿½é—®è´¨é‡ï¼š[åˆ†æAIçš„è¿½é—®æ˜¯å¦æ°å½“ã€æ·±å…¥]
3. ä¿¡æ¯è·å–ï¼š[åˆ†æAIæ˜¯å¦æˆåŠŸè·å–äº†è§£å†³é—®é¢˜æ‰€éœ€çš„å…³é”®ä¿¡æ¯]
4. æ”¹è¿›å»ºè®®ï¼š[é’ˆå¯¹è¯¥ç”¨æˆ·ç”»åƒçš„å…·ä½“æ”¹è¿›å»ºè®®]
""",
            
            "answer_correctness": f"""
{context_section}

è¯·è¯¦ç»†åˆ†æä¸Šè¿°å®Œæ•´å¯¹è¯ä¸­AIå›ç­”çš„å‡†ç¡®æ€§ä¸ä¸“ä¸šæ€§ï¼Œå¹¶ç»™å‡º1-5åˆ†è¯„åˆ†ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
5åˆ†ï¼šå›ç­”å®Œå…¨å‡†ç¡®ï¼Œä¸“ä¸šæ€§å¼ºï¼Œç¬¦åˆè¡Œä¸šæ ‡å‡†
4åˆ†ï¼šå›ç­”åŸºæœ¬å‡†ç¡®ï¼Œæœ‰ä¸€å®šä¸“ä¸šæ€§
3åˆ†ï¼šå›ç­”éƒ¨åˆ†å‡†ç¡®ï¼Œä¸“ä¸šæ€§ä¸€èˆ¬
2åˆ†ï¼šå›ç­”å‡†ç¡®æ€§æœ‰é™ï¼Œä¸“ä¸šæ€§ä¸è¶³
1åˆ†ï¼šå›ç­”ä¸å‡†ç¡®ï¼Œç¼ºä¹ä¸“ä¸šæ€§

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
è¯„åˆ†ï¼šX/5
è¯¦ç»†åˆ†æï¼š
1. å‡†ç¡®æ€§è¯„ä¼°ï¼š[åˆ†æAIå›ç­”çš„äº‹å®å‡†ç¡®æ€§]
2. ä¸“ä¸šæ€§è¯„ä¼°ï¼š[åˆ†æAIå›ç­”æ˜¯å¦ç¬¦åˆ{persona.get('role', 'ä¸“ä¸šç”¨æˆ·')}çš„ä¸“ä¸šè¦æ±‚]
3. å®Œæ•´æ€§è¯„ä¼°ï¼š[åˆ†æAIå›ç­”æ˜¯å¦å®Œæ•´è§£å†³äº†ç”¨æˆ·é—®é¢˜]
4. æ”¹è¿›å»ºè®®ï¼š[é’ˆå¯¹è¯¥ç”¨æˆ·ç”»åƒçš„å…·ä½“æ”¹è¿›å»ºè®®]
""",
            
            "persona_alignment": f"""
{context_section}

è¯·è¯¦ç»†åˆ†æä¸Šè¿°å®Œæ•´å¯¹è¯ä¸­AIä¸ç”¨æˆ·ç”»åƒçš„åŒ¹é…åº¦ï¼Œå¹¶ç»™å‡º1-5åˆ†è¯„åˆ†ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
5åˆ†ï¼šå®Œå…¨åŒ¹é…ç”¨æˆ·ç”»åƒï¼Œæ²Ÿé€šé£æ ¼å’Œä¸“ä¸šåº¦é«˜åº¦å¥‘åˆ
4åˆ†ï¼šåŸºæœ¬åŒ¹é…ç”¨æˆ·ç”»åƒï¼Œæ²Ÿé€šè¾ƒä¸ºæ°å½“
3åˆ†ï¼šéƒ¨åˆ†åŒ¹é…ç”¨æˆ·ç”»åƒï¼Œæ²Ÿé€šé£æ ¼ä¸€èˆ¬
2åˆ†ï¼šåŒ¹é…åº¦æœ‰é™ï¼Œæ²Ÿé€šé£æ ¼ä¸å¤Ÿæ°å½“
1åˆ†ï¼šä¸åŒ¹é…ç”¨æˆ·ç”»åƒï¼Œæ²Ÿé€šé£æ ¼ä¸åˆé€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
è¯„åˆ†ï¼šX/5
è¯¦ç»†åˆ†æï¼š
1. è§’è‰²åŒ¹é…ï¼š[åˆ†æAIæ˜¯å¦ç†è§£ç”¨æˆ·çš„{persona.get('role', 'ä¸“ä¸šç”¨æˆ·')}èº«ä»½]
2. æ²Ÿé€šé£æ ¼ï¼š[åˆ†æAIçš„æ²Ÿé€šæ–¹å¼æ˜¯å¦ç¬¦åˆç”¨æˆ·çš„{persona.get('communication_style', 'ä¸“ä¸šæ²Ÿé€š')}åå¥½]
3. ä¸“ä¸šå¥‘åˆï¼š[åˆ†æAIçš„å›ç­”æ˜¯å¦ç¬¦åˆ{context.get('business_domain', 'ä¸“ä¸šæœåŠ¡')}é¢†åŸŸè¦æ±‚]
4. æ”¹è¿›å»ºè®®ï¼š[é’ˆå¯¹è¯¥ç”¨æˆ·ç”»åƒçš„å…·ä½“æ”¹è¿›å»ºè®®]
""",
            
            "goal_alignment": f"""
{context_section}

è¯·è¯¦ç»†åˆ†æä¸Šè¿°å®Œæ•´å¯¹è¯ä¸­AIä¸ç”¨æˆ·ç›®æ ‡çš„å¯¹é½åº¦ï¼Œå¹¶ç»™å‡º1-5åˆ†è¯„åˆ†ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
5åˆ†ï¼šå®Œå…¨ç†è§£å¹¶æ»¡è¶³ç”¨æˆ·ç›®æ ‡ï¼Œä¸»åŠ¨æä¾›ç›¸å…³å¸®åŠ©
4åˆ†ï¼šåŸºæœ¬ç†è§£ç”¨æˆ·ç›®æ ‡ï¼Œæä¾›æœ‰æ•ˆå¸®åŠ©
3åˆ†ï¼šéƒ¨åˆ†ç†è§£ç”¨æˆ·ç›®æ ‡ï¼Œå¸®åŠ©æœ‰é™
2åˆ†ï¼šå¯¹ç”¨æˆ·ç›®æ ‡ç†è§£ä¸è¶³ï¼Œå¸®åŠ©ä¸å¤Ÿ
1åˆ†ï¼šä¸ç†è§£ç”¨æˆ·ç›®æ ‡ï¼Œæ— æ³•æä¾›æœ‰æ•ˆå¸®åŠ©

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
è¯„åˆ†ï¼šX/5
è¯¦ç»†åˆ†æï¼š
1. ç›®æ ‡ç†è§£ï¼š[åˆ†æAIæ˜¯å¦å‡†ç¡®ç†è§£äº†ç”¨æˆ·çš„çœŸå®éœ€æ±‚å’Œç›®æ ‡]
2. è§£å†³æ•ˆæœï¼š[åˆ†æAIçš„å›ç­”æ˜¯å¦æœ‰æ•ˆè§£å†³äº†ç”¨æˆ·é—®é¢˜]
3. ä¸»åŠ¨æ€§ï¼š[åˆ†æAIæ˜¯å¦ä¸»åŠ¨æä¾›äº†ç›¸å…³çš„é¢å¤–å¸®åŠ©]
4. æ”¹è¿›å»ºè®®ï¼š[é’ˆå¯¹è¯¥ç”¨æˆ·ç”»åƒçš„å…·ä½“æ”¹è¿›å»ºè®®]
"""
        }
        
        evaluation_results = {}
        detailed_explanations = {}
        
        # Evaluate each dimension with detailed analysis
        for dimension, prompt in evaluation_prompts.items():
            try:
                print(f"  ğŸ“Š è¯„ä¼° {dimension} (åœºæ™¯: {scenario_info.get('title', 'æœªçŸ¥')[:20]}...)...")
                response = await call_deepseek_with_strict_timeout(prompt)
                
                # Extract score and detailed explanation
                score = extract_score_from_response(response)
                detailed_explanation = extract_detailed_explanation(response)
                
                evaluation_results[dimension] = score
                detailed_explanations[dimension] = detailed_explanation
                
                print(f"  âœ… {dimension}: {score}/5 (åŒ¹é…{persona.get('role', 'ç”¨æˆ·')}éœ€æ±‚)")
                
            except Exception as e:
                print(f"  âŒ Failed to evaluate {dimension}: {str(e)}")
                raise Exception(f"è¯„ä¼°ç»´åº¦ {dimension} å¤±è´¥: {str(e)}")
        
        # Calculate overall score
        scenario_score = sum(evaluation_results.values()) / len(evaluation_results)
        print(f"ğŸ¯ åœºæ™¯æ•´ä½“å¾—åˆ†: {scenario_score:.2f}/5.0")
        
        return evaluation_results, detailed_explanations, scenario_score
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
        raise Exception(f"DeepSeekè¯„ä¼°å¤±è´¥: {str(e)}")

def extract_detailed_explanation(response: str) -> Dict[str, str]:
    """
    Extract detailed explanation from DeepSeek response
    """
    try:
        explanation = {
            "score": "æœªçŸ¥",
            "detailed_analysis": "æœªæä¾›è¯¦ç»†åˆ†æ",
            "specific_points": []
        }
        
        lines = response.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if line.startswith('è¯„åˆ†ï¼š'):
                explanation["score"] = line.replace('è¯„åˆ†ï¼š', '').strip()
            elif line.startswith('è¯¦ç»†åˆ†æï¼š'):
                current_section = "analysis"
            elif line and current_section == "analysis":
                if any(line.startswith(f"{i}.") for i in range(1, 10)):
                    explanation["specific_points"].append(line)
                elif line.startswith(('1.', '2.', '3.', '4.')):
                    explanation["specific_points"].append(line)
        
        # Combine all specific points into detailed analysis
        if explanation["specific_points"]:
            explanation["detailed_analysis"] = "\n".join(explanation["specific_points"])
        
        return explanation
        
    except Exception as e:
        print(f"Warning: Could not parse detailed explanation: {str(e)}")
        return {
            "score": "æœªçŸ¥",
            "detailed_analysis": response[:500] + "..." if len(response) > 500 else response,
            "specific_points": []
        }

async def generate_single_initial_message(scenario_info: Dict, user_persona_info: Dict) -> str:
    """Generate ONLY the opening message - no full conversation"""
    persona = user_persona_info.get('user_persona', {})
    context = user_persona_info.get('usage_context', {})
    
    prompt = f"""
ä½œä¸º{persona.get('role', 'ç”¨æˆ·')}ï¼Œç”Ÿæˆä¸€ä¸ªè‡ªç„¶çš„å¼€åœºé—®é¢˜ã€‚

ç”¨æˆ·èº«ä»½: {persona.get('role', 'ç”¨æˆ·')}
å·¥ä½œç»éªŒ: {persona.get('experience_level', 'ä¸­ç­‰ç»éªŒ')}
å·¥ä½œç¯å¢ƒ: {persona.get('work_environment', 'å·¥ä½œåœºæ‰€')}
æ²Ÿé€šé£æ ¼: {persona.get('communication_style', 'ä¸“ä¸š')}
ä¸šåŠ¡é¢†åŸŸ: {context.get('business_domain', 'ä¸“ä¸šæœåŠ¡')}

å¯¹è¯åœºæ™¯: {scenario_info.get('title', 'ä¸“ä¸šå’¨è¯¢')}
åœºæ™¯èƒŒæ™¯: {scenario_info.get('context', 'å·¥ä½œåœºæ™¯')}

è¯·ç”Ÿæˆä¸€ä¸ªç¬¦åˆ{persona.get('role', 'ç”¨æˆ·')}èº«ä»½çš„å¼€åœºé—®é¢˜æˆ–æè¿°ï¼Œä½“ç°{scenario_info.get('title', 'åœºæ™¯')}çš„ç‰¹ç‚¹ã€‚

è¦æ±‚:
- åƒçœŸå®çš„{persona.get('role', 'ç”¨æˆ·')}æé—®
- ä½“ç°{persona.get('experience_level', 'ç»éªŒæ°´å¹³')}
- ç¬¦åˆ{scenario_info.get('title', 'åœºæ™¯')}èƒŒæ™¯
- åªè¿”å›ä¸€å¥è¯ï¼Œä¸è¦è§£é‡Š

ç¤ºä¾‹æ ¼å¼: "ç°åœºå‘ç°...", "éœ€è¦ç¡®è®¤...", "å…³äº...è§„èŒƒè¦æ±‚"
"""

    try:
        response = await call_deepseek_with_strict_timeout(prompt)
        if response and len(response.strip()) > 5:
            return response.strip().strip('"').strip("'")
    except Exception as e:
        print(f"âš ï¸ DeepSeekç”Ÿæˆåˆå§‹æ¶ˆæ¯å¤±è´¥: {str(e)}")
    
    # Simple fallback based on scenario
    role = persona.get('role', 'ç”¨æˆ·')
    if 'å·¥ç¨‹å¸ˆ' in role or 'ç›‘ç†' in role:
        return f"ç°åœº{scenario_info.get('title', 'æ–½å·¥')}æœ‰æŠ€æœ¯é—®é¢˜éœ€è¦ç¡®è®¤"
    else:
        return f"å…³äº{scenario_info.get('title', 'ä¸šåŠ¡')}æœ‰é—®é¢˜å’¨è¯¢"

async def generate_next_message_based_on_response(
    scenario_info: Dict, 
    user_persona_info: Dict, 
    conversation_history: List[Dict], 
    coze_response: str
) -> str:
    """Generate next message based ONLY on Coze's actual response"""
    persona = user_persona_info.get('user_persona', {})
    
    # Analyze Coze's response characteristics
    response_analysis = analyze_coze_response(coze_response)
    
    prompt = f"""
ä½œä¸º{persona.get('role', 'ç”¨æˆ·')}ï¼ŒåŸºäºAIçš„å›å¤ï¼Œç”Ÿæˆè‡ªç„¶çš„åç»­æ¶ˆæ¯ã€‚

ä½ çš„èº«ä»½: {persona.get('role', 'ç”¨æˆ·')}
æ²Ÿé€šé£æ ¼: {persona.get('communication_style', 'ä¸“ä¸š')}
ç»éªŒæ°´å¹³: {persona.get('experience_level', 'ä¸­ç­‰')}

AIåˆšæ‰çš„å›å¤:
{coze_response[:400]}

å›å¤ç‰¹å¾åˆ†æ:
- æ˜¯å¦æä¾›äº†å…·ä½“ä¿¡æ¯: {response_analysis['has_specific_info']}
- æ˜¯å¦åŒ…å«è§„èŒƒå¼•ç”¨: {response_analysis['has_standards']}
- å›å¤å®Œæ•´åº¦: {response_analysis['completeness']}

è¯·ç”Ÿæˆç¬¦åˆ{persona.get('role', 'ç”¨æˆ·')}èº«ä»½çš„åç»­æ¶ˆæ¯:
1. å¦‚æœAIå›ç­”å®Œæ•´ä¸”è§£å†³é—®é¢˜ â†’ è¡¨è¾¾æ„Ÿè°¢å¹¶ç»“æŸ("è°¢è°¢ï¼Œé—®é¢˜è§£å†³äº†")
2. å¦‚æœAIå›ç­”æ¨¡ç³Š â†’ è¿½é—®å…·ä½“ç»†èŠ‚
3. å¦‚æœAIæä¾›é“¾æ¥ä½†éœ€è¦å…·ä½“å†…å®¹ â†’ è¯¢é—®å…³é”®æ¡æ–‡
4. å¦‚æœAIå›ç­”éƒ¨åˆ†æ­£ç¡® â†’ è¡¥å……è¿½é—®ç›¸å…³é—®é¢˜

åªè¿”å›åç»­æ¶ˆæ¯å†…å®¹ï¼Œå¦‚éœ€ç»“æŸå¯¹è¯è¿”å›"END"ã€‚
"""

    try:
        response = await call_deepseek_with_strict_timeout(prompt)
        if response:
            message = response.strip().strip('"').strip("'")
            if message.upper() in ["END", "FINISH", "DONE"]:
                return ""
            return message if len(message) > 3 else ""
    except Exception:
        pass
    
    # Smart fallback based on turn number and AI response quality
    turn_num = len(conversation_history)
    
    if turn_num >= 4:  # Natural ending after several turns
        return ""
    
    # Generate contextual response based on role and AI response content
    role = persona.get('role', 'ç”¨æˆ·')
    ai_response_lower = coze_response.lower()
    
    if 'è§„èŒƒ' in ai_response_lower and ('å·¥ç¨‹å¸ˆ' in role or 'ç›‘ç†' in role):
        return "å…·ä½“æ¡æ–‡å·æ˜¯ä»€ä¹ˆï¼Ÿç°åœºæ“ä½œè¦æ³¨æ„å“ªäº›è¦ç‚¹ï¼Ÿ"
    elif 'æ ‡å‡†' in ai_response_lower or 'è¦æ±‚' in ai_response_lower:
        return "è¿™ä¸ªæ ‡å‡†çš„å…·ä½“æ‰§è¡Œç»†èŠ‚æ˜¯ä»€ä¹ˆï¼Ÿ"
    elif 'å¯ä»¥' in ai_response_lower or 'å»ºè®®' in ai_response_lower:
        return "å¥½çš„ï¼Œè¿˜æœ‰å…¶ä»–éœ€è¦æ³¨æ„çš„å—ï¼Ÿ"
    elif len(coze_response) < 100:  # Short response, need more details
        return "èƒ½è¯¦ç»†è¯´æ˜ä¸€ä¸‹å…·ä½“æ“ä½œæ­¥éª¤å—ï¼Ÿ"
    else:
        return "æ˜ç™½äº†ï¼Œè°¢è°¢"

def analyze_coze_response(response: str) -> Dict[str, bool]:
    """Analyze Coze response characteristics"""
    return {
        'has_specific_info': len(response) > 100 and ('æ¡' in response or 'ç¬¬' in response or 'GB' in response),
        'has_standards': 'è§„èŒƒ' in response or 'æ ‡å‡†' in response or 'GB' in response,
        'completeness': len(response) > 150
    }

async def call_coze_with_strict_timeout(api_config: APIConfig, message: str) -> str:
    """Call Coze API with strict 2-minute timeout and no excessive retries"""
    timeout_seconds = 120  # 2 minutes total
    max_retries = 2  # Only 2 attempts max
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ”„ Cozeè°ƒç”¨ (å°è¯• {attempt + 1}/{max_retries})")
            
            async with httpx.AsyncClient(timeout=httpx.Timeout(timeout_seconds)) as client:
                if api_config.type == 'coze-agent':
                    return await call_coze_agent_with_timeout(api_config, message, client)
                elif api_config.type == 'coze-bot' or hasattr(api_config, 'botId'):
                    return await call_coze_bot_with_timeout(api_config, message, client)
                else:
                    return await call_generic_api_with_timeout(api_config, message, client)
                    
        except asyncio.TimeoutError:
            print(f"â° ç¬¬{attempt + 1}æ¬¡è°ƒç”¨è¶…æ—¶ (2åˆ†é’Ÿ)")
            if attempt < max_retries - 1:
                continue
            else:
                raise Exception("Coze APIè°ƒç”¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–Agenté…ç½®")
        except Exception as e:
            print(f"âŒ ç¬¬{attempt + 1}æ¬¡è°ƒç”¨å¤±è´¥: {str(e)}")
            if attempt < max_retries - 1:
                continue
            else:
                raise Exception(f"Coze APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    raise Exception("Coze APIè°ƒç”¨å®Œå…¨å¤±è´¥")

async def call_deepseek_with_strict_timeout(prompt: str) -> str:
    """Call DeepSeek API with strict timeout and no retries"""
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,
        "max_tokens": 300
    }
    
    try:
        async with httpx.AsyncClient(timeout=httpx.Timeout(30.0)) as client:
            response = await client.post(DEEPSEEK_API_URL, headers=headers, json=payload)
            
            if response.status_code == 200:
                result = response.json()
                if "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0].get("message", {}).get("content", "").strip()
                    if content and len(content) > 5:
                        return content
            
            raise Exception(f"DeepSeek APIé”™è¯¯: {response.status_code}")
            
    except Exception as e:
        raise Exception(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}")

async def call_coze_agent_with_timeout(api_config: APIConfig, message: str, client: httpx.AsyncClient) -> str:
    """Call Coze Agent API with timeout - no excessive polling"""
    base_url = "https://api.coze.cn" if api_config.region == "china" else "https://api.coze.com"
    chat_url = f"{base_url}/v3/chat"
    
    headers = {
        "Authorization": f"Bearer {api_config.headers.get('Authorization', '').replace('Bearer ', '')}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "bot_id": api_config.agentId,
        "user_id": f"user_{datetime.now().strftime('%H%M%S')}",
        "stream": False,
        "auto_save_history": True,
        "additional_messages": [{"role": "user", "content": message, "content_type": "text"}]
    }
    
    # Start conversation
    response = await client.post(chat_url, headers=headers, json=payload)
    if response.status_code != 200:
        raise Exception(f"Agent APIé”™è¯¯: {response.status_code}")
    
    result = response.json()
    if result.get("code") != 0:
        raise Exception(f"Agentå“åº”é”™è¯¯: {result.get('msg', 'Unknown error')}")
    
    data = result.get("data", {})
    conversation_id = data.get("conversation_id")
    chat_id = data.get("id")
    status = data.get("status")
    
    if status == "completed":
        # Get messages immediately
        return await get_agent_messages(client, base_url, headers, conversation_id, chat_id)
    elif status == "in_progress":
        # Limited polling - max 20 attempts with 3-second intervals (1 minute total)
        messages_url = f"{base_url}/v1/conversation/message/list"
        for poll in range(20):
            await asyncio.sleep(3)
            
            params = {"conversation_id": conversation_id, "chat_id": chat_id}
            messages_response = await client.get(messages_url, headers=headers, params=params)
            
            if messages_response.status_code == 200:
                messages_result = messages_response.json()
                if messages_result.get("code") == 0:
                    messages = messages_result.get("data", [])
                    for msg in messages:
                        if msg.get("role") == "assistant" and msg.get("content"):
                            return msg.get("content", "").strip()
        
        raise Exception("Agentå¤„ç†è¶…æ—¶ (1åˆ†é’Ÿ)")
    else:
        raise Exception(f"AgentçŠ¶æ€å¼‚å¸¸: {status}")

async def get_agent_messages(client: httpx.AsyncClient, base_url: str, headers: Dict, conversation_id: str, chat_id: str) -> str:
    """Get agent messages with single call"""
    messages_url = f"{base_url}/v1/conversation/message/list"
    params = {"conversation_id": conversation_id, "chat_id": chat_id}
    
    response = await client.get(messages_url, headers=headers, params=params)
    if response.status_code != 200:
        raise Exception(f"è·å–æ¶ˆæ¯å¤±è´¥: {response.status_code}")
    
    result = response.json()
    if result.get("code") != 0:
        raise Exception(f"æ¶ˆæ¯APIé”™è¯¯: {result.get('msg')}")
    
    messages = result.get("data", [])
    for msg in messages:
        if msg.get("role") == "assistant" and msg.get("content"):
            return msg.get("content", "").strip()
    
    raise Exception("æœªæ‰¾åˆ°Assistantå›å¤")

async def call_coze_bot_with_timeout(api_config: APIConfig, message: str, client: httpx.AsyncClient) -> str:
    """Call Coze Bot API with timeout"""
    url = "https://api.coze.cn/open_api/v2/chat"
    headers = {
        "Authorization": "Bearer pat_aWWxLQe20D8km5FsKt5W99pWL72L5LNxjkontH91q3lqqTU0ExBKUBl1cUy4tm8c",
        "Content-Type": "application/json"
    }
    
    payload = {
        "conversation_id": f"conv_{datetime.now().strftime('%H%M%S')}",
        "bot_id": getattr(api_config, 'botId', '7511993619423985674'),
        "user": "test_user",
        "query": message,
        "stream": False
    }
    
    response = await client.post(url, headers=headers, json=payload)
    if response.status_code != 200:
        raise Exception(f"Bot APIé”™è¯¯: {response.status_code}")
    
    result = response.json()
    messages = result.get("messages", [])
    
    for msg in messages:
        if msg.get("type") == "answer" and msg.get("content"):
            return msg.get("content", "").strip()
    
    raise Exception("Botæœªè¿”å›æœ‰æ•ˆå›å¤")

async def call_generic_api_with_timeout(api_config: APIConfig, message: str, client: httpx.AsyncClient) -> str:
    """Call generic API with timeout"""
    headers = api_config.headers.copy()
    headers.setdefault("Content-Type", "application/json")
    
    payload = {"message": message, "query": message}
    
    response = await client.request(
        method=api_config.method,
        url=api_config.url,
        headers=headers,
        json=payload
    )
    
    if response.status_code != 200:
        raise Exception(f"APIé”™è¯¯: {response.status_code}")
    
    result = response.json()
    return result.get("response", result.get("answer", result.get("message", str(result))))

async def generate_final_comprehensive_report(
    evaluation_results: List[Dict], 
    user_persona_info: Dict, 
    requirement_context: str
) -> Dict:
    """
    Generate comprehensive final report for dynamic evaluation with extracted persona/context display
    """
    if not evaluation_results:
        return {
            "overall_analysis": "è¯„ä¼°å¤±è´¥ï¼šæ— æœ‰æ•ˆå¯¹è¯æ•°æ®",
            "extracted_persona_summary": user_persona_info,
            "cross_scenario_insights": [],
            "persona_alignment_analysis": "æ— æ³•åˆ†æ",
            "improvement_recommendations": ["è¯·æ£€æŸ¥AI Agenté…ç½®å’Œç½‘ç»œè¿æ¥"],
            "detailed_metrics": {}
        }
    
    # Extract persona information for prominent display
    persona = user_persona_info.get('user_persona', {})
    context = user_persona_info.get('usage_context', {})
    requirements = user_persona_info.get('extracted_requirements', {})
    
    # Calculate comprehensive metrics
    all_scores = []
    total_turns = 0
    scenario_summaries = []
    dimension_averages = {}
    
    for result in evaluation_results:
        scores = result.get('evaluation_scores', {})
        if scores:
            all_scores.extend(scores.values())
            # Calculate dimension averages
            for dimension, score in scores.items():
                if dimension not in dimension_averages:
                    dimension_averages[dimension] = []
                dimension_averages[dimension].append(score)
        
        total_turns += result.get('total_turns', 0)
        
        scenario_summaries.append({
            "title": result.get('scenario_title', 'æœªå‘½ååœºæ™¯'),
            "score": result.get('scenario_score', 0),
            "turns": result.get('total_turns', 0),
            "key_strengths": extract_strengths_from_scores(scores),
            "improvement_areas": extract_weaknesses_from_scores(scores),
            "detailed_evaluations": result.get('evaluation_explanations', {})
        })
    
    # Calculate final dimension averages
    final_dimension_averages = {}
    for dimension, scores in dimension_averages.items():
        final_dimension_averages[dimension] = sum(scores) / len(scores) if scores else 0
    
    overall_score = sum(all_scores) / len(all_scores) if all_scores else 0
    
    # Generate enhanced analysis prompt with extracted persona emphasis
    analysis_prompt = f"""
åŸºäºä»¥ä¸‹åŠ¨æ€å¯¹è¯è¯„ä¼°ç»“æœå’Œæå–çš„ç”¨æˆ·ç”»åƒï¼Œç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Šï¼š

=== ä»éœ€æ±‚æ–‡æ¡£æå–çš„ç”¨æˆ·ç”»åƒä¿¡æ¯ ===
ç”¨æˆ·è§’è‰²: {persona.get('role', 'æœªæŒ‡å®š')}
ç»éªŒæ°´å¹³: {persona.get('experience_level', 'æœªæŒ‡å®š')}
ä¸“ä¸šé¢†åŸŸ: {', '.join(persona.get('expertise_areas', ['æœªæŒ‡å®š']))}
æ²Ÿé€šé£æ ¼: {persona.get('communication_style', 'æœªæŒ‡å®š')}
å·¥ä½œç¯å¢ƒ: {persona.get('work_environment', 'æœªæŒ‡å®š')}

ä¸šåŠ¡ä¸Šä¸‹æ–‡: {context.get('business_domain', 'æœªæŒ‡å®š')}
ä¸»è¦ä½¿ç”¨åœºæ™¯: {', '.join(context.get('primary_scenarios', ['æœªæŒ‡å®š']))}
äº¤äº’ç›®æ ‡: {', '.join(context.get('interaction_goals', ['æœªæŒ‡å®š']))}
å…³é”®ç—›ç‚¹: {', '.join(context.get('pain_points', ['æœªæŒ‡å®š']))}

æ ¸å¿ƒåŠŸèƒ½éœ€æ±‚: {', '.join(requirements.get('core_functions', ['æœªæŒ‡å®š']))}
è´¨é‡æœŸæœ›: {', '.join(requirements.get('quality_expectations', ['æœªæŒ‡å®š']))}

=== è¯„ä¼°ç»“æœæ•°æ® ===
æ€»ä½“å¾—åˆ†: {overall_score:.2f}/5.0
è¯„ä¼°åœºæ™¯æ•°: {len(evaluation_results)}
æ€»å¯¹è¯è½®æ¬¡: {total_turns}

ç»´åº¦å¾—åˆ†:
- æ¨¡ç³Šç†è§£èƒ½åŠ›: {final_dimension_averages.get('fuzzy_understanding', 0):.2f}/5.0
- å›ç­”å‡†ç¡®æ€§: {final_dimension_averages.get('answer_correctness', 0):.2f}/5.0
- ç”¨æˆ·åŒ¹é…åº¦: {final_dimension_averages.get('persona_alignment', 0):.2f}/5.0
- ç›®æ ‡å¯¹é½åº¦: {final_dimension_averages.get('goal_alignment', 0):.2f}/5.0

åœºæ™¯è¯¦æƒ…:
{chr(10).join([f"åœºæ™¯{i+1}: {s['title']} - å¾—åˆ†{s['score']:.1f}/5.0 ({s['turns']}è½®å¯¹è¯)" for i, s in enumerate(scenario_summaries)])}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„ç»¼åˆåˆ†æï¼š
1. æ•´ä½“è¡¨ç°è¯„ä»·ï¼šåŸºäºæå–çš„{persona.get('role', 'ç”¨æˆ·è§’è‰²')}éœ€æ±‚ï¼ŒAIçš„æ•´ä½“è¡¨ç°å¦‚ä½•ï¼Ÿï¼ˆ150å­—å†…ï¼‰
2. ç”¨æˆ·ç”»åƒåŒ¹é…åº¦åˆ†æï¼šAIæ˜¯å¦å¾ˆå¥½åœ°é€‚åº”äº†æå–çš„ç”¨æˆ·è§’è‰²å’Œæ²Ÿé€šé£æ ¼ï¼Ÿï¼ˆ100å­—å†…ï¼‰
3. ä¸šåŠ¡ç›®æ ‡è¾¾æˆåº¦ï¼šæ˜¯å¦æ»¡è¶³äº†ä»éœ€æ±‚æ–‡æ¡£ä¸­æå–çš„ä¸šåŠ¡ç›®æ ‡å’ŒåŠŸèƒ½éœ€æ±‚ï¼Ÿï¼ˆ100å­—å†…ï¼‰
4. è·¨åœºæ™¯å¯¹æ¯”æ´å¯Ÿï¼šä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°å·®å¼‚å’Œè§„å¾‹ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰
5. é’ˆå¯¹æ€§æ”¹è¿›å»ºè®®ï¼šåŸºäºæå–çš„ç”¨æˆ·ç”»åƒå’Œä¸šåŠ¡éœ€æ±‚çš„å…·ä½“æ”¹è¿›å»ºè®®ï¼ˆ5-8æ¡ï¼‰

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€ï¼Œé‡ç‚¹çªå‡ºä¸æå–çš„ç”¨æˆ·ç”»åƒå’Œéœ€æ±‚çš„åŒ¹é…æƒ…å†µã€‚
"""

    try:
        comprehensive_analysis = await call_deepseek_api_with_fallback(analysis_prompt)
        
        if comprehensive_analysis and comprehensive_analysis != "APIè¯„ä¼°æš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨è¯„åˆ†æœºåˆ¶":
            # Parse the analysis response
            analysis_sections = parse_enhanced_comprehensive_analysis(comprehensive_analysis)
        else:
            # Fallback analysis with persona emphasis
            analysis_sections = generate_enhanced_fallback_analysis(overall_score, scenario_summaries, user_persona_info)
            
    except Exception as e:
        print(f"âš ï¸ ç»¼åˆåˆ†æç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨åˆ†æ: {str(e)[:50]}...")
        analysis_sections = generate_enhanced_fallback_analysis(overall_score, scenario_summaries, user_persona_info)
    
    # Generate persona-specific recommendations
    recommendations = generate_persona_specific_recommendations(evaluation_results, user_persona_info, final_dimension_averages)
    
    return {
        "overall_analysis": analysis_sections.get("overall_evaluation", "æ•´ä½“è¡¨ç°éœ€è¦è¿›ä¸€æ­¥åˆ†æ"),
        "extracted_persona_summary": {
            "user_persona": persona,
            "usage_context": context,
            "extracted_requirements": requirements,
            "ai_role_simulation": user_persona_info.get('ai_role_simulation', {}),
            "extraction_source": "DeepSeekæ™ºèƒ½æå–"
        },
        "persona_alignment_analysis": analysis_sections.get("persona_alignment", "ç”¨æˆ·ç”»åƒåŒ¹é…åº¦æœ‰å¾…è¯„ä¼°"),
        "business_goal_achievement": analysis_sections.get("business_goals", "ä¸šåŠ¡ç›®æ ‡è¾¾æˆåº¦éœ€è¦åˆ†æ"),
        "cross_scenario_insights": analysis_sections.get("cross_scenario_insights", []),
        "improvement_recommendations": recommendations,
        "detailed_metrics": {
            "overall_score": overall_score,
            "dimension_scores": final_dimension_averages,
            "total_scenarios": len(evaluation_results),
            "total_conversation_turns": total_turns,
            "average_turns_per_scenario": total_turns / len(evaluation_results) if evaluation_results else 0,
            "scenario_summaries": scenario_summaries
        },
        "evaluation_timestamp": datetime.now().isoformat(),
        "evaluation_mode": "dynamic_conversation_with_extracted_persona"
    }

def extract_strengths_from_scores(scores: Dict) -> List[str]:
    """Extract strengths based on evaluation scores"""
    strengths = []
    for dimension, score in scores.items():
        if score >= 4.0:
            if dimension == "fuzzy_understanding":
                strengths.append("æ¨¡ç³Šç†è§£èƒ½åŠ›å¼º")
            elif dimension == "answer_correctness":
                strengths.append("å›ç­”å‡†ç¡®ä¸“ä¸š")
            elif dimension == "persona_alignment":
                strengths.append("ç”¨æˆ·åŒ¹é…åº¦é«˜")
            elif dimension == "goal_alignment":
                strengths.append("ç›®æ ‡å¯¹é½è‰¯å¥½")
    return strengths

def extract_weaknesses_from_scores(scores: Dict) -> List[str]:
    """Extract improvement areas based on evaluation scores"""
    weaknesses = []
    for dimension, score in scores.items():
        if score < 3.5:
            if dimension == "fuzzy_understanding":
                weaknesses.append("éœ€åŠ å¼ºè¿½é—®å¼•å¯¼")
            elif dimension == "answer_correctness":
                weaknesses.append("ä¸“ä¸šå‡†ç¡®æ€§å¾…æå‡")
            elif dimension == "persona_alignment":
                weaknesses.append("ç”¨æˆ·é€‚é…åº¦éœ€æ”¹å–„")
            elif dimension == "goal_alignment":
                weaknesses.append("ç›®æ ‡å¯¹é½éœ€ä¼˜åŒ–")
    return weaknesses

def parse_enhanced_comprehensive_analysis(analysis_text: str) -> Dict:
    """Parse comprehensive analysis response into structured sections"""
    sections = {
        "overall_evaluation": "",
        "cross_scenario_insights": [],
        "persona_analysis": "",
        "business_goals": ""
    }
    
    lines = analysis_text.split('\n')
    current_section = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if "æ•´ä½“è¡¨ç°" in line or "æ€»ä½“è¯„ä»·" in line:
            current_section = "overall_evaluation"
        elif "è·¨åœºæ™¯" in line or "å¯¹æ¯”æ´å¯Ÿ" in line:
            current_section = "cross_scenario_insights"
        elif "ç”¨æˆ·ç”»åƒ" in line or "åŒ¹é…åº¦" in line:
            current_section = "persona_analysis"
        elif "ä¸šåŠ¡ç›®æ ‡" in line or "ä¸šåŠ¡è¾¾æˆ" in line:
            current_section = "business_goals"
        elif current_section == "overall_evaluation" and not sections["overall_evaluation"]:
            sections["overall_evaluation"] = line
        elif current_section == "cross_scenario_insights" and line.startswith(('â€¢', '-', '1.', '2.', '3.', '4.', '5.')):
            sections["cross_scenario_insights"].append(line.lstrip('â€¢-123456789. '))
        elif current_section == "persona_analysis" and not sections["persona_analysis"]:
            sections["persona_analysis"] = line
        elif current_section == "business_goals" and not sections["business_goals"]:
            sections["business_goals"] = line
    
    return sections

def generate_enhanced_fallback_analysis(overall_score: float, scenario_summaries: List[Dict], user_persona_info: Dict) -> Dict:
    """Generate fallback analysis when DeepSeek API is unavailable"""
    persona = user_persona_info.get('user_persona', {})
    
    if overall_score >= 4.0:
        overall_eval = f"AI Agentæ•´ä½“è¡¨ç°ä¼˜ç§€ï¼ˆ{overall_score:.1f}/5.0ï¼‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†{persona.get('role', 'ç”¨æˆ·')}çš„ä¸“ä¸šéœ€æ±‚"
    elif overall_score >= 3.0:
        overall_eval = f"AI Agentè¡¨ç°è‰¯å¥½ï¼ˆ{overall_score:.1f}/5.0ï¼‰ï¼ŒåŸºæœ¬æ»¡è¶³ä½¿ç”¨éœ€æ±‚ï¼Œéƒ¨åˆ†ç¯èŠ‚æœ‰æ”¹è¿›ç©ºé—´"
    else:
        overall_eval = f"AI Agentè¡¨ç°éœ€è¦æ”¹è¿›ï¼ˆ{overall_score:.1f}/5.0ï¼‰ï¼Œå»ºè®®é‡ç‚¹ä¼˜åŒ–å¯¹è¯ç­–ç•¥å’Œä¸“ä¸šçŸ¥è¯†"
    
    insights = [
        f"å…±å®Œæˆ{len(scenario_summaries)}ä¸ªåœºæ™¯çš„åŠ¨æ€å¯¹è¯æµ‹è¯•",
        f"å¹³å‡æ¯åœºæ™¯{sum(s['turns'] for s in scenario_summaries) / len(scenario_summaries):.1f}è½®å¯¹è¯",
        "åŠ¨æ€é—®é¢˜ç”Ÿæˆæœºåˆ¶è¿è¡Œæ­£å¸¸" if scenario_summaries else "å¯¹è¯ç”Ÿæˆéœ€è¦ä¼˜åŒ–"
    ]
    
    persona_analysis = f"ä¸{persona.get('role', 'ç”¨æˆ·')}è§’è‰²çš„åŒ¹é…åº¦æ•´ä½“{('è‰¯å¥½' if overall_score >= 3.5 else 'æœ‰å¾…æå‡')}"
    
    business_goals = f"åŸºäºæå–çš„éœ€æ±‚æ–‡æ¡£ï¼Œä¸šåŠ¡ç›®æ ‡è¾¾æˆåº¦{('è¾ƒå¥½' if overall_score >= 3.5 else 'éœ€è¦æ”¹è¿›')}"
    
    return {
        "overall_evaluation": overall_eval,
        "cross_scenario_insights": insights,
        "persona_alignment": persona_analysis,
        "business_goals": business_goals
    }

def generate_persona_specific_recommendations(evaluation_results: List[Dict], user_persona_info: Dict, dimension_averages: Dict) -> List[str]:
    """
    Generate persona-specific recommendations based on extracted user persona and evaluation results
    """
    recommendations = []
    
    if not evaluation_results:
        return [
            "æ— æ³•ç”Ÿæˆæ¨èå»ºè®®ï¼Œè¯·å…ˆå®Œæˆæœ‰æ•ˆçš„è¯„ä¼°",
            "æ£€æŸ¥AI Agenté…ç½®å’Œç½‘ç»œè¿æ¥",
            "ç¡®ä¿å¯¹è¯åœºæ™¯é…ç½®æ­£ç¡®"
        ]
    
    persona = user_persona_info.get('user_persona', {})
    context = user_persona_info.get('usage_context', {})
    requirements = user_persona_info.get('extracted_requirements', {})
    
    # Overall performance assessment based on extracted persona
    overall_avg = sum(dimension_averages.values()) / len(dimension_averages) if dimension_averages else 0
    
    if overall_avg >= 4.5:
        recommendations.append(f"ğŸŸ¢ é’ˆå¯¹{persona.get('role', 'ç”¨æˆ·')}çš„æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼AIä»£ç†èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†{context.get('business_domain', 'ä¸šåŠ¡')}éœ€æ±‚")
    elif overall_avg >= 4.0:
        recommendations.append(f"ğŸŸ¡ å¯¹{persona.get('role', 'ç”¨æˆ·')}çš„æœåŠ¡è‰¯å¥½ï¼ŒåŸºæœ¬æ»¡è¶³{context.get('business_domain', 'ä¸šåŠ¡')}éœ€æ±‚ï¼Œæœ‰è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´")
    elif overall_avg >= 3.0:
        recommendations.append(f"ğŸŸ  æœåŠ¡{persona.get('role', 'ç”¨æˆ·')}çš„èƒ½åŠ›ä¸­ç­‰ï¼Œå»ºè®®é’ˆå¯¹{context.get('business_domain', 'ä¸šåŠ¡é¢†åŸŸ')}ç‰¹ç‚¹è¿›è¡Œæ”¹è¿›")
    else:
        recommendations.append(f"ğŸ”´ éœ€è¦æ˜¾è‘—æ”¹è¿›å¯¹{persona.get('role', 'ç”¨æˆ·')}çš„æœåŠ¡èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯{context.get('business_domain', 'ä¸šåŠ¡')}ç›¸å…³åŠŸèƒ½")
    
    # Dimension-specific recommendations with persona context
    if dimension_averages.get('fuzzy_understanding', 0) < 3.5:
        pain_points = context.get('pain_points', [])
        pain_context = f"ï¼Œç‰¹åˆ«æ˜¯{', '.join(pain_points[:2])}" if pain_points else ""
        recommendations.append(f"ğŸ’¡ é’ˆå¯¹{persona.get('role', 'ç”¨æˆ·')}çš„æ¨¡ç³Šç†è§£èƒ½åŠ›éœ€è¦åŠ å¼ºï¼šå¢åŠ è¿½é—®å¼•å¯¼æœºåˆ¶{pain_context}")
    
    if dimension_averages.get('answer_correctness', 0) < 3.5:
        expertise_areas = persona.get('expertise_areas', [])
        expertise_context = f"ï¼Œç‰¹åˆ«æ˜¯{', '.join(expertise_areas[:2])}é¢†åŸŸ" if expertise_areas else ""
        recommendations.append(f"ğŸ“š é’ˆå¯¹{persona.get('role', 'ç”¨æˆ·')}çš„ä¸“ä¸šå‡†ç¡®æ€§éœ€è¦æå‡ï¼šåŠ å¼ºçŸ¥è¯†åº“å»ºè®¾{expertise_context}")
    
    if dimension_averages.get('persona_alignment', 0) < 3.5:
        comm_style = persona.get('communication_style', 'ä¸“ä¸šæ²Ÿé€š')
        recommendations.append(f"ğŸ‘¥ ç”¨æˆ·åŒ¹é…åº¦æœ‰å¾…æ”¹å–„ï¼šä¼˜åŒ–è¯­è¨€é£æ ¼ä»¥é€‚åº”{comm_style}ï¼ŒåŒ¹é…{persona.get('experience_level', 'ç”¨æˆ·ç»éªŒæ°´å¹³')}")
    
    if dimension_averages.get('goal_alignment', 0) < 3.5:
        core_functions = requirements.get('core_functions', [])
        function_context = f"ï¼Œé‡ç‚¹å…³æ³¨{', '.join(core_functions[:2])}" if core_functions else ""
        recommendations.append(f"ğŸ¯ ä¸šåŠ¡ç›®æ ‡å¯¹é½åº¦éœ€è¦æ”¹è¿›ï¼šç¡®ä¿å›ç­”èƒ½å¤Ÿæ»¡è¶³{context.get('business_domain', 'ä¸šåŠ¡')}çš„å®é™…éœ€æ±‚{function_context}")
    
    # Add persona-specific targeted recommendations
    role = persona.get('role', '')
    work_env = persona.get('work_environment', '')
    
    if 'å®¢æœ' in role:
        recommendations.append(f"ğŸ§ å®¢æœåœºæ™¯ä¼˜åŒ–ï¼šé’ˆå¯¹{work_env}ç¯å¢ƒï¼Œæå‡å“åº”æ•ˆç‡å’Œæ ‡å‡†åŒ–å›ç­”")
        quality_expectations = requirements.get('quality_expectations', [])
        if quality_expectations:
            recommendations.append(f"â±ï¸ æœåŠ¡è´¨é‡æå‡ï¼šé‡ç‚¹æ»¡è¶³{', '.join(quality_expectations[:2])}ç­‰å®¢æœè´¨é‡è¦æ±‚")
    elif 'å·¥ç¨‹å¸ˆ' in role or 'ç›‘ç†' in role:
        recommendations.append(f"ğŸ”§ æŠ€æœ¯ä¸“ä¸šæ€§ï¼šåŠ å¼ºå¯¹{work_env}ç¯å¢ƒä¸‹æŠ€æœ¯è§„èŒƒå’Œæ ‡å‡†çš„æ”¯æŒ")
        if 'è§„èŒƒ' in str(context.get('primary_scenarios', [])):
            recommendations.append("ğŸ“‹ è§„èŒƒæŸ¥è¯¢ä¼˜åŒ–ï¼šå¢å¼ºå¯¹æŠ€æœ¯æ ‡å‡†å’Œæ–½å·¥è§„èŒƒçš„å¿«é€Ÿæ£€ç´¢å’Œè§£é‡Šèƒ½åŠ›")
    elif 'ç®¡ç†' in role:
        recommendations.append("ğŸ“Š ç®¡ç†å†³ç­–æ”¯æŒï¼šæä¾›æ›´å¤šæ•°æ®åˆ†æå’Œå†³ç­–å»ºè®®åŠŸèƒ½")
    
    # Add interaction preference recommendations
    interaction_goals = context.get('interaction_goals', [])
    if interaction_goals:
        recommendations.append(f"ğŸ¯ äº¤äº’ç›®æ ‡ä¼˜åŒ–ï¼šé‡ç‚¹æå‡{', '.join(interaction_goals[:2])}çš„å®ç°æ•ˆæœ")
    
    # Quality expectations based recommendations  
    quality_expectations = requirements.get('quality_expectations', [])
    if quality_expectations:
        recommendations.append(f"â­ è´¨é‡æ ‡å‡†å¯¹é½ï¼šç¡®ä¿è¾¾åˆ°{', '.join(quality_expectations[:2])}ç­‰è´¨é‡æœŸæœ›")
    
    # Limit to 6-8 most relevant recommendations
    unique_recommendations = list(dict.fromkeys(recommendations))
    return unique_recommendations[:8]

def extract_recommendations_from_response(response: str) -> List[str]:
    """Extract improvement recommendations from DeepSeek response"""
    try:
        # Look for numbered or bulleted recommendations
        recommendations = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            if any(marker in line for marker in ['1.', '2.', '3.', '4.', '5.', '-', 'â€¢', 'â‘ ', 'â‘¡', 'â‘¢']):
                # Clean the line and extract recommendation
                clean_rec = re.sub(r'^[\d\.\-\â€¢â‘ â‘¡â‘¢â‘£â‘¤]\s*', '', line).strip()
                if clean_rec and len(clean_rec) > 10:
                    recommendations.append(clean_rec)
        
        # If no structured recommendations found, try to extract from content
        if not recommendations and response:
            # Split into sentences and look for actionable suggestions
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.]', response)
            for sentence in sentences:
                if any(keyword in sentence for keyword in ['å»ºè®®', 'åº”è¯¥', 'éœ€è¦', 'å¯ä»¥', 'æ”¹è¿›', 'æå‡', 'ä¼˜åŒ–']):
                    if len(sentence.strip()) > 15:
                        recommendations.append(sentence.strip())
        
        return recommendations[:5] if recommendations else [
            "å¢å¼ºå¯¹è¯ç†è§£èƒ½åŠ›",
            "æé«˜å›ç­”å‡†ç¡®æ€§",
            "ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ",
            "åŠ å¼ºä¸“ä¸šçŸ¥è¯†æ·±åº¦"
        ]
        
    except Exception:
        return [
            "å¢å¼ºå¯¹è¯ç†è§£èƒ½åŠ›",
            "æé«˜å›ç­”å‡†ç¡®æ€§", 
            "ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ",
            "åŠ å¼ºä¸“ä¸šçŸ¥è¯†æ·±åº¦"
        ]

@app.post("/api/validate-config")
async def validate_agent_config(agent_api_config: str = Form(...)):
    """
    Validate AI Agent configuration before evaluation
    """
    try:
        # Parse configuration
        api_config_dict = json.loads(agent_api_config)
        api_config = APIConfig(**api_config_dict)
        
        validation_results = {
            "is_valid": True,
            "warnings": [],
            "errors": [],
            "suggestions": []
        }
        
        # Basic validation
        if not api_config.url:
            validation_results["errors"].append("API URLä¸èƒ½ä¸ºç©º")
            validation_results["is_valid"] = False
        
        if not api_config.headers:
            validation_results["warnings"].append("æœªè®¾ç½®è¯·æ±‚å¤´ï¼Œå¯èƒ½å¯¼è‡´è®¤è¯å¤±è´¥")
        
        # Coze-specific validation
        if api_config.type == "coze-agent":
            if not api_config.agentId:
                validation_results["errors"].append("Coze Agent IDä¸èƒ½ä¸ºç©º")
                validation_results["is_valid"] = False
            
            auth_header = api_config.headers.get("Authorization", "")
            if not auth_header or not auth_header.startswith("Bearer "):
                validation_results["errors"].append("Coze Agentéœ€è¦æœ‰æ•ˆçš„Bearer Token")
                validation_results["is_valid"] = False
            
            if api_config.region not in ["global", "china"]:
                validation_results["warnings"].append("å»ºè®®ä½¿ç”¨ 'global' æˆ– 'china' ä½œä¸ºregion")
        
        elif api_config.type == "coze-bot":
            if not api_config.botId:
                validation_results["errors"].append("Coze Bot IDä¸èƒ½ä¸ºç©º")
                validation_results["is_valid"] = False
        
        # Test connectivity (optional quick test)
        if validation_results["is_valid"]:
            try:
                # Quick connectivity test with timeout
                async with httpx.AsyncClient(timeout=httpx.Timeout(5.0)) as client:
                    if api_config.type == "coze-agent":
                        # Test Coze Agent endpoint
                        test_url = f"https://api.coze.{'cn' if api_config.region == 'china' else 'com'}/v3/chat"
                        response = await client.post(
                            test_url,
                            headers=api_config.headers,
                            json={
                                "bot_id": api_config.agentId,
                                "user_id": "test_validation",
                                "stream": False,
                                "auto_save_history": False,
                                "additional_messages": [
                                    {"role": "user", "content": "test", "content_type": "text"}
                                ]
                            }
                        )
                        
                        if response.status_code == 401:
                            validation_results["errors"].append("è®¤è¯å¤±è´¥ï¼šè¯·æ£€æŸ¥Access Tokenæ˜¯å¦æœ‰æ•ˆ")
                            validation_results["is_valid"] = False
                        elif response.status_code == 403:
                            validation_results["errors"].append("æƒé™ä¸è¶³ï¼šè¯·æ£€æŸ¥Tokenæƒé™æˆ–Agent ID")
                            validation_results["is_valid"] = False
                        elif response.status_code in [200, 400]:  # 400 might be expected for test message
                            validation_results["suggestions"].append("âœ… APIè¿æ¥æµ‹è¯•æˆåŠŸ")
                        else:
                            validation_results["warnings"].append(f"APIè¿”å›çŠ¶æ€ç : {response.status_code}")
                    
                    else:
                        # Test custom API endpoint
                        response = await client.post(
                            api_config.url,
                            headers=api_config.headers,
                            json={"message": "test"}
                        )
                        
                        if response.status_code in [200, 400, 401]:
                            validation_results["suggestions"].append("âœ… APIç«¯ç‚¹å¯è®¿é—®")
                        else:
                            validation_results["warnings"].append(f"APIè¿”å›çŠ¶æ€ç : {response.status_code}")
                            
            except httpx.TimeoutException:
                validation_results["warnings"].append("APIè¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–URL")
            except Exception as e:
                validation_results["warnings"].append(f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)[:50]}...")
        
        # Add helpful suggestions
        if validation_results["is_valid"]:
            validation_results["suggestions"].extend([
                "ğŸ’¡ å»ºè®®å…ˆç”¨ç®€å•é—®é¢˜æµ‹è¯•AI Agentå“åº”",
                "ğŸ“ ç¡®ä¿Agentå·²å‘å¸ƒä¸”å¤„äºæ´»è·ƒçŠ¶æ€",
                "ğŸ”„ å¦‚é‡åˆ°é—®é¢˜ï¼Œå¯å°è¯•é‡æ–°ç”ŸæˆAccess Token"
            ])
        
        return {
            "validation_result": validation_results,
            "config_summary": {
                "type": api_config.type,
                "url": api_config.url,
                "has_auth": bool(api_config.headers.get("Authorization")),
                "timeout": api_config.timeout
            }
        }
        
    except json.JSONDecodeError:
        return {
            "validation_result": {
                "is_valid": False,
                "errors": ["é…ç½®æ ¼å¼é”™è¯¯ï¼šè¯·æ£€æŸ¥JSONæ ¼å¼"],
                "warnings": [],
                "suggestions": ["è¯·ç¡®ä¿é…ç½®ä¿¡æ¯ä¸ºæœ‰æ•ˆçš„JSONæ ¼å¼"]
            }
        }
    except Exception as e:
        return {
            "validation_result": {
                "is_valid": False,
                "errors": [f"é…ç½®éªŒè¯å¤±è´¥: {str(e)}"],
                "warnings": [],
                "suggestions": ["è¯·æ£€æŸ¥é…ç½®ä¿¡æ¯æ˜¯å¦å®Œæ•´"]
            }
        }

async def call_deepseek_api_with_fallback(prompt: str, max_retries: int = 2) -> str:
    """Enhanced DeepSeek API call with better error handling and suppressed exceptions"""
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,
        "max_tokens": 500
    }
    
    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(20.0)) as client:
                response = await client.post(DEEPSEEK_API_URL, headers=headers, json=payload)
                
                if response.status_code == 200:
                    result = response.json()
                    if "choices" in result and len(result["choices"]) > 0:
                        content = result["choices"][0].get("message", {}).get("content", "").strip()
                        if content and len(content) > 10:
                            return content
                
                if attempt < max_retries - 1:
                    continue
                    
        except Exception as e:
            if attempt < max_retries - 1:
                continue
    
    return "APIè¯„ä¼°æš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨è¯„åˆ†æœºåˆ¶"

# Create alias function to redirect to new implementation
async def conduct_dynamic_conversation(api_config: APIConfig, scenario_info: Dict, user_persona_info: Dict) -> List[Dict]:
    """Redirect to true dynamic conversation implementation"""
    return await conduct_true_dynamic_conversation(api_config, scenario_info, user_persona_info)

async def conduct_true_dynamic_conversation(api_config: APIConfig, scenario_info: Dict, user_persona_info: Dict) -> List[Dict]:
    """
    TRUE dynamic conversation: DeepSeek generates one message at a time based on Coze's actual responses
    No pre-generated scenarios - pure turn-by-turn interaction
    """
    print(f"ğŸ—£ï¸ å¼€å§‹çœŸæ­£åŠ¨æ€å¯¹è¯: {scenario_info.get('title', 'æœªå‘½ååœºæ™¯')}")
    
    conversation_history = []
    persona = user_persona_info.get('user_persona', {})
    context = user_persona_info.get('usage_context', {})
    
    # Step 1: Generate ONLY the initial message based on persona and scenario
    print("ğŸ¯ DeepSeekç”Ÿæˆåˆå§‹æ¶ˆæ¯...")
    initial_message = await generate_single_initial_message(scenario_info, user_persona_info)
    if not initial_message:
        raise Exception("Failed to generate initial message")
    
    current_user_message = initial_message
    
    # Step 2: Conduct true turn-by-turn conversation (max 5 turns)
    for turn_num in range(1, 6):
        print(f"ğŸ’¬ ç¬¬ {turn_num} è½® - ç”¨æˆ·æ¶ˆæ¯: {current_user_message[:50]}...")
        
        # Add persona context for better AI understanding
        enhanced_message = f"[ä½œä¸º{persona.get('role', 'ç”¨æˆ·')}ï¼Œ{persona.get('communication_style', 'ä¸“ä¸šæ²Ÿé€š')}] {current_user_message}"
        
        # Get Coze response with strict timeout
        coze_response = await call_coze_with_strict_timeout(api_config, enhanced_message)
        if not coze_response:
            print(f"âŒ ç¬¬ {turn_num} è½®Cozeæ— å“åº”ï¼Œç»ˆæ­¢å¯¹è¯")
            break
            
        # Record this turn
        conversation_history.append({
            "turn": turn_num,
            "user_message": current_user_message,
            "enhanced_message": enhanced_message,
            "ai_response": coze_response,
            "response_length": len(coze_response),
            "timestamp": datetime.now().isoformat()
        })
        
        print(f"âœ… Cozeå“åº”: {coze_response[:80]}...")
        
        # Generate next message based on Coze's actual response
        if turn_num < 5:  # Don't generate after last turn
            print(f"ğŸ¤– DeepSeekåˆ†æCozeå›å¤ï¼Œç”Ÿæˆç¬¬{turn_num + 1}è½®æ¶ˆæ¯...")
            next_message = await generate_next_message_based_on_response(
                scenario_info, user_persona_info, conversation_history, coze_response
            )
            
            if not next_message or next_message.upper() in ["END", "FINISH", "DONE"]:
                print(f"ğŸ”š å¯¹è¯è‡ªç„¶ç»“æŸäºç¬¬ {turn_num} è½®")
                break
                
            current_user_message = next_message
        
    print(f"ğŸ“Š åŠ¨æ€å¯¹è¯å®Œæˆï¼Œå…± {len(conversation_history)} è½®")
    return conversation_history

if __name__ == "__main__":
    port = find_available_port()
    print(f"ğŸš€ AI Agentè¯„ä¼°å¹³å°å¯åŠ¨åœ¨ç«¯å£ {port}")
    uvicorn.run(app, host="0.0.0.0", port=port) 